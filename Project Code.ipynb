{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Cardiac Arrhythmia Multi-Class Classification \n",
    "\n",
    "Analyze data and address missing data if there is any. \n",
    "\n",
    "Decide aboute a good evaluation strategy and justify your choice. \n",
    "\n",
    "Find the best parameters for the following classification models: \n",
    "- KNN classifcation \n",
    "- Logistic Regression\n",
    "- Linear Supprt Vector Machine\n",
    "- Kerenilzed Support Vector Machine\n",
    "- Decision Tree\n",
    "- Random Forest \n",
    "\n",
    "Use of different bagging and boosting methods to boost the results.  \n",
    "\n",
    "Use of data reduction methods to reduce the size of data, and agian try above models. Do get better results? J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kFold = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cardiac_arrhythmia.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>91</td>\n",
       "      <td>193</td>\n",
       "      <td>371</td>\n",
       "      <td>174</td>\n",
       "      <td>121</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64</td>\n",
       "      <td>81</td>\n",
       "      <td>174</td>\n",
       "      <td>401</td>\n",
       "      <td>149</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>95</td>\n",
       "      <td>138</td>\n",
       "      <td>163</td>\n",
       "      <td>386</td>\n",
       "      <td>185</td>\n",
       "      <td>102</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>94</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>380</td>\n",
       "      <td>179</td>\n",
       "      <td>143</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>181</td>\n",
       "      <td>360</td>\n",
       "      <td>177</td>\n",
       "      <td>103</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   270   271  272  273  \\\n",
       "0   75    0  190   80   91  193  371  174  121  -16 ...   0.0   9.0 -0.9  0.0   \n",
       "1   56    1  165   64   81  174  401  149   39   25 ...   0.0   8.5  0.0  0.0   \n",
       "2   54    0  172   95  138  163  386  185  102   96 ...   0.0   9.5 -2.4  0.0   \n",
       "3   55    0  175   94  100  202  380  179  143   28 ...   0.0  12.2 -2.2  0.0   \n",
       "4   75    0  190   80   88  181  360  177  103  -16 ...   0.0  13.1 -3.6  0.0   \n",
       "\n",
       "  274  275  276   277   278  279  \n",
       "0   0  0.9  2.9  23.3  49.4    8  \n",
       "1   0  0.2  2.1  20.4  38.8    6  \n",
       "2   0  0.3  3.4  12.3  49.0   10  \n",
       "3   0  0.4  2.6  34.6  61.6    1  \n",
       "4   0 -0.1  3.9  25.4  62.8    7  \n",
       "\n",
       "[5 rows x 280 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452, 280)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       8\n",
       "1       6\n",
       "2      10\n",
       "3       1\n",
       "4       7\n",
       "5      14\n",
       "6       1\n",
       "7       1\n",
       "8       1\n",
       "9      10\n",
       "10      3\n",
       "11      1\n",
       "12     10\n",
       "13      6\n",
       "14      1\n",
       "15      1\n",
       "16     10\n",
       "17      1\n",
       "18      1\n",
       "19      1\n",
       "20      1\n",
       "21      1\n",
       "22      1\n",
       "23      1\n",
       "24      1\n",
       "25     16\n",
       "26     14\n",
       "27     10\n",
       "28      2\n",
       "29      2\n",
       "       ..\n",
       "422     1\n",
       "423     1\n",
       "424     9\n",
       "425     1\n",
       "426     1\n",
       "427    10\n",
       "428     1\n",
       "429    16\n",
       "430    10\n",
       "431     6\n",
       "432    10\n",
       "433     3\n",
       "434     1\n",
       "435     1\n",
       "436     1\n",
       "437     1\n",
       "438     1\n",
       "439     1\n",
       "440     1\n",
       "441     1\n",
       "442     1\n",
       "443    10\n",
       "444     1\n",
       "445     1\n",
       "446     1\n",
       "447     1\n",
       "448    10\n",
       "449     2\n",
       "450     1\n",
       "451     1\n",
       "Name: 279, Length: 452, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[279]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in range(0,452):\n",
    "    for j in range(0,280):\n",
    "        if (data.iloc[i,j]=='?'):\n",
    "            count =count+1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing '?' with Null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,452):\n",
    "    for j in range(0,280):\n",
    "        if (data.iloc[i,j]=='?'):\n",
    "            data.iloc[i,j] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "5        0\n",
       "6        0\n",
       "7        0\n",
       "8        0\n",
       "9        0\n",
       "10       8\n",
       "11      22\n",
       "12       1\n",
       "13     376\n",
       "14       1\n",
       "15       0\n",
       "16       0\n",
       "17       0\n",
       "18       0\n",
       "19       0\n",
       "20       0\n",
       "21       0\n",
       "22       0\n",
       "23       0\n",
       "24       0\n",
       "25       0\n",
       "26       0\n",
       "27       0\n",
       "28       0\n",
       "29       0\n",
       "      ... \n",
       "250      0\n",
       "251      0\n",
       "252      0\n",
       "253      0\n",
       "254      0\n",
       "255      0\n",
       "256      0\n",
       "257      0\n",
       "258      0\n",
       "259      0\n",
       "260      0\n",
       "261      0\n",
       "262      0\n",
       "263      0\n",
       "264      0\n",
       "265      0\n",
       "266      0\n",
       "267      0\n",
       "268      0\n",
       "269      0\n",
       "270      0\n",
       "271      0\n",
       "272      0\n",
       "273      0\n",
       "274      0\n",
       "275      0\n",
       "276      0\n",
       "277      0\n",
       "278      0\n",
       "279      0\n",
       "Length: 280, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(data).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droping column 13 with lots of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = 13, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing other missing data using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/452 with 0 missing, elapsed time: 0.178\n",
      "Imputing row 101/452 with 0 missing, elapsed time: 0.179\n",
      "Imputing row 201/452 with 1 missing, elapsed time: 0.179\n",
      "Imputing row 301/452 with 1 missing, elapsed time: 0.180\n",
      "Imputing row 401/452 with 0 missing, elapsed time: 0.180\n"
     ]
    }
   ],
   "source": [
    "import fancyimpute\n",
    "data_no_missing = fancyimpute.KNN(k=5).complete(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(data_no_missing).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_missing = pd.DataFrame(data_no_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1      2     3      4      5      6      7      8     9    ...   269  \\\n",
       "0  75.0  0.0  190.0  80.0   91.0  193.0  371.0  174.0  121.0 -16.0  ...   0.0   \n",
       "1  56.0  1.0  165.0  64.0   81.0  174.0  401.0  149.0   39.0  25.0  ...   0.0   \n",
       "2  54.0  0.0  172.0  95.0  138.0  163.0  386.0  185.0  102.0  96.0  ...   0.0   \n",
       "3  55.0  0.0  175.0  94.0  100.0  202.0  380.0  179.0  143.0  28.0  ...   0.0   \n",
       "4  75.0  0.0  190.0  80.0   88.0  181.0  360.0  177.0  103.0 -16.0  ...   0.0   \n",
       "\n",
       "    270  271  272  273  274  275   276   277   278  \n",
       "0   9.0 -0.9  0.0  0.0  0.9  2.9  23.3  49.4   8.0  \n",
       "1   8.5  0.0  0.0  0.0  0.2  2.1  20.4  38.8   6.0  \n",
       "2   9.5 -2.4  0.0  0.0  0.3  3.4  12.3  49.0  10.0  \n",
       "3  12.2 -2.2  0.0  0.0  0.4  2.6  34.6  61.6   1.0  \n",
       "4  13.1 -3.6  0.0  0.0 -0.1  3.9  25.4  62.8   7.0  \n",
       "\n",
       "[5 rows x 279 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_missing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0     245\n",
       "10.0     50\n",
       "2.0      44\n",
       "6.0      25\n",
       "16.0     22\n",
       "4.0      15\n",
       "3.0      15\n",
       "5.0      13\n",
       "9.0       9\n",
       "15.0      5\n",
       "14.0      4\n",
       "7.0       3\n",
       "8.0       2\n",
       "Name: 278, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_missing[278].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.059042</td>\n",
       "      <td>-0.109458</td>\n",
       "      <td>0.381555</td>\n",
       "      <td>-0.004032</td>\n",
       "      <td>0.041149</td>\n",
       "      <td>0.195691</td>\n",
       "      <td>0.025654</td>\n",
       "      <td>0.099755</td>\n",
       "      <td>-0.265868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164816</td>\n",
       "      <td>-0.158009</td>\n",
       "      <td>0.082376</td>\n",
       "      <td>0.090413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.037877</td>\n",
       "      <td>-0.271504</td>\n",
       "      <td>0.018043</td>\n",
       "      <td>-0.199728</td>\n",
       "      <td>-0.092381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.059042</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.124685</td>\n",
       "      <td>-0.248104</td>\n",
       "      <td>-0.337101</td>\n",
       "      <td>-0.046771</td>\n",
       "      <td>0.072052</td>\n",
       "      <td>-0.184736</td>\n",
       "      <td>-0.081051</td>\n",
       "      <td>0.069434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230938</td>\n",
       "      <td>-0.042638</td>\n",
       "      <td>0.092879</td>\n",
       "      <td>0.027401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.065684</td>\n",
       "      <td>0.031688</td>\n",
       "      <td>0.046605</td>\n",
       "      <td>-0.178080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.109458</td>\n",
       "      <td>-0.124685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.074957</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>-0.237314</td>\n",
       "      <td>-0.038411</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>0.061539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018876</td>\n",
       "      <td>-0.073439</td>\n",
       "      <td>-0.091361</td>\n",
       "      <td>-0.002545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.067670</td>\n",
       "      <td>-0.008471</td>\n",
       "      <td>-0.090370</td>\n",
       "      <td>-0.092235</td>\n",
       "      <td>0.006648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.381555</td>\n",
       "      <td>-0.248104</td>\n",
       "      <td>-0.074957</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100094</td>\n",
       "      <td>0.119826</td>\n",
       "      <td>0.118657</td>\n",
       "      <td>0.149987</td>\n",
       "      <td>0.120668</td>\n",
       "      <td>-0.173355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050471</td>\n",
       "      <td>-0.026926</td>\n",
       "      <td>0.051946</td>\n",
       "      <td>0.047448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.046278</td>\n",
       "      <td>-0.144721</td>\n",
       "      <td>0.062285</td>\n",
       "      <td>-0.050682</td>\n",
       "      <td>-0.090151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.004032</td>\n",
       "      <td>-0.337101</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>0.100094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021831</td>\n",
       "      <td>0.218681</td>\n",
       "      <td>0.397435</td>\n",
       "      <td>0.049682</td>\n",
       "      <td>-0.146043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198941</td>\n",
       "      <td>0.091384</td>\n",
       "      <td>-0.228688</td>\n",
       "      <td>-0.013697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.065596</td>\n",
       "      <td>-0.222170</td>\n",
       "      <td>0.129796</td>\n",
       "      <td>-0.082791</td>\n",
       "      <td>0.323879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.041149</td>\n",
       "      <td>-0.046771</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.119826</td>\n",
       "      <td>0.021831</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079371</td>\n",
       "      <td>0.074618</td>\n",
       "      <td>0.670865</td>\n",
       "      <td>-0.012412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005347</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>-0.074180</td>\n",
       "      <td>0.068594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.143272</td>\n",
       "      <td>0.061160</td>\n",
       "      <td>-0.027691</td>\n",
       "      <td>0.021048</td>\n",
       "      <td>-0.099954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.195691</td>\n",
       "      <td>0.072052</td>\n",
       "      <td>-0.237314</td>\n",
       "      <td>0.118657</td>\n",
       "      <td>0.218681</td>\n",
       "      <td>0.079371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166711</td>\n",
       "      <td>0.063044</td>\n",
       "      <td>-0.031786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035215</td>\n",
       "      <td>0.121349</td>\n",
       "      <td>0.116718</td>\n",
       "      <td>-0.002351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.034971</td>\n",
       "      <td>-0.038884</td>\n",
       "      <td>0.256200</td>\n",
       "      <td>0.150979</td>\n",
       "      <td>0.028305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.025654</td>\n",
       "      <td>-0.184736</td>\n",
       "      <td>-0.038411</td>\n",
       "      <td>0.149987</td>\n",
       "      <td>0.397435</td>\n",
       "      <td>0.074618</td>\n",
       "      <td>0.166711</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060302</td>\n",
       "      <td>-0.099283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068035</td>\n",
       "      <td>0.070535</td>\n",
       "      <td>-0.046868</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.049175</td>\n",
       "      <td>-0.184846</td>\n",
       "      <td>0.130202</td>\n",
       "      <td>-0.014430</td>\n",
       "      <td>0.097625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.099755</td>\n",
       "      <td>-0.081051</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>0.120668</td>\n",
       "      <td>0.049682</td>\n",
       "      <td>0.670865</td>\n",
       "      <td>0.063044</td>\n",
       "      <td>0.060302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.062760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010142</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>-0.091258</td>\n",
       "      <td>0.096772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.248092</td>\n",
       "      <td>0.016054</td>\n",
       "      <td>-0.016365</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>-0.122003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.265868</td>\n",
       "      <td>0.069434</td>\n",
       "      <td>0.061539</td>\n",
       "      <td>-0.173355</td>\n",
       "      <td>-0.146043</td>\n",
       "      <td>-0.012412</td>\n",
       "      <td>-0.031786</td>\n",
       "      <td>-0.099283</td>\n",
       "      <td>-0.062760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179725</td>\n",
       "      <td>0.292155</td>\n",
       "      <td>0.295283</td>\n",
       "      <td>-0.071495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.082009</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.297780</td>\n",
       "      <td>0.255203</td>\n",
       "      <td>0.019585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.008095</td>\n",
       "      <td>-0.141667</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>-0.046153</td>\n",
       "      <td>0.031114</td>\n",
       "      <td>0.114645</td>\n",
       "      <td>0.159971</td>\n",
       "      <td>0.040838</td>\n",
       "      <td>0.143195</td>\n",
       "      <td>-0.051925</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034564</td>\n",
       "      <td>-0.052905</td>\n",
       "      <td>-0.055454</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.071090</td>\n",
       "      <td>0.157539</td>\n",
       "      <td>-0.062546</td>\n",
       "      <td>0.084336</td>\n",
       "      <td>0.005178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.046294</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>-0.106477</td>\n",
       "      <td>-0.026709</td>\n",
       "      <td>0.040207</td>\n",
       "      <td>0.008069</td>\n",
       "      <td>-0.091543</td>\n",
       "      <td>-0.111296</td>\n",
       "      <td>0.054635</td>\n",
       "      <td>0.054649</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007202</td>\n",
       "      <td>-0.065499</td>\n",
       "      <td>0.063194</td>\n",
       "      <td>-0.003007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.212522</td>\n",
       "      <td>0.039697</td>\n",
       "      <td>-0.036134</td>\n",
       "      <td>-0.013432</td>\n",
       "      <td>-0.012897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.285805</td>\n",
       "      <td>0.030865</td>\n",
       "      <td>0.024969</td>\n",
       "      <td>-0.212601</td>\n",
       "      <td>-0.075626</td>\n",
       "      <td>0.020685</td>\n",
       "      <td>-0.084250</td>\n",
       "      <td>-0.107960</td>\n",
       "      <td>-0.073149</td>\n",
       "      <td>0.709528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222315</td>\n",
       "      <td>0.169013</td>\n",
       "      <td>0.132601</td>\n",
       "      <td>-0.066043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.052213</td>\n",
       "      <td>0.194113</td>\n",
       "      <td>0.101686</td>\n",
       "      <td>0.214848</td>\n",
       "      <td>0.069403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.192239</td>\n",
       "      <td>0.062729</td>\n",
       "      <td>0.286553</td>\n",
       "      <td>-0.174988</td>\n",
       "      <td>-0.006001</td>\n",
       "      <td>-0.045091</td>\n",
       "      <td>-0.654681</td>\n",
       "      <td>0.016251</td>\n",
       "      <td>0.043433</td>\n",
       "      <td>0.011351</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051391</td>\n",
       "      <td>-0.118857</td>\n",
       "      <td>-0.236425</td>\n",
       "      <td>0.032815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.179483</td>\n",
       "      <td>-0.083605</td>\n",
       "      <td>-0.249300</td>\n",
       "      <td>-0.234168</td>\n",
       "      <td>0.008331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.016675</td>\n",
       "      <td>-0.129383</td>\n",
       "      <td>-0.001940</td>\n",
       "      <td>0.038332</td>\n",
       "      <td>0.118847</td>\n",
       "      <td>0.017437</td>\n",
       "      <td>0.018153</td>\n",
       "      <td>0.024361</td>\n",
       "      <td>0.072590</td>\n",
       "      <td>-0.055314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.342547</td>\n",
       "      <td>-0.118037</td>\n",
       "      <td>-0.028179</td>\n",
       "      <td>-0.042085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.086201</td>\n",
       "      <td>-0.037790</td>\n",
       "      <td>-0.170610</td>\n",
       "      <td>-0.163450</td>\n",
       "      <td>0.003965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.199533</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>-0.091314</td>\n",
       "      <td>0.117731</td>\n",
       "      <td>0.310412</td>\n",
       "      <td>-0.067438</td>\n",
       "      <td>0.296504</td>\n",
       "      <td>0.251603</td>\n",
       "      <td>-0.025343</td>\n",
       "      <td>-0.271659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195220</td>\n",
       "      <td>0.126898</td>\n",
       "      <td>0.199933</td>\n",
       "      <td>0.094711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.103729</td>\n",
       "      <td>-0.241676</td>\n",
       "      <td>0.484570</td>\n",
       "      <td>0.159676</td>\n",
       "      <td>0.042674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.120405</td>\n",
       "      <td>-0.149390</td>\n",
       "      <td>0.040893</td>\n",
       "      <td>-0.023805</td>\n",
       "      <td>0.185647</td>\n",
       "      <td>0.053189</td>\n",
       "      <td>-0.084368</td>\n",
       "      <td>-0.037765</td>\n",
       "      <td>0.035735</td>\n",
       "      <td>0.213609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112477</td>\n",
       "      <td>0.025226</td>\n",
       "      <td>-0.302136</td>\n",
       "      <td>-0.081102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008272</td>\n",
       "      <td>0.099483</td>\n",
       "      <td>-0.267412</td>\n",
       "      <td>-0.102082</td>\n",
       "      <td>0.195198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.028642</td>\n",
       "      <td>-0.020588</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>0.119989</td>\n",
       "      <td>0.051225</td>\n",
       "      <td>0.012168</td>\n",
       "      <td>0.014332</td>\n",
       "      <td>0.013292</td>\n",
       "      <td>-0.011175</td>\n",
       "      <td>0.055497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037438</td>\n",
       "      <td>0.034874</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>-0.007184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.008735</td>\n",
       "      <td>0.040192</td>\n",
       "      <td>0.060671</td>\n",
       "      <td>0.073527</td>\n",
       "      <td>0.063999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.152401</td>\n",
       "      <td>-0.088119</td>\n",
       "      <td>-0.082112</td>\n",
       "      <td>0.145418</td>\n",
       "      <td>0.415102</td>\n",
       "      <td>-0.034179</td>\n",
       "      <td>0.233446</td>\n",
       "      <td>0.287783</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>-0.258226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053776</td>\n",
       "      <td>0.126330</td>\n",
       "      <td>0.039008</td>\n",
       "      <td>-0.007327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.003834</td>\n",
       "      <td>-0.198374</td>\n",
       "      <td>0.309029</td>\n",
       "      <td>0.072924</td>\n",
       "      <td>0.042764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.044455</td>\n",
       "      <td>0.042517</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.118853</td>\n",
       "      <td>0.024790</td>\n",
       "      <td>-0.163104</td>\n",
       "      <td>-0.103370</td>\n",
       "      <td>0.018588</td>\n",
       "      <td>-0.164281</td>\n",
       "      <td>0.005523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023960</td>\n",
       "      <td>0.084864</td>\n",
       "      <td>0.034307</td>\n",
       "      <td>-0.003746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.137654</td>\n",
       "      <td>-0.033795</td>\n",
       "      <td>0.060650</td>\n",
       "      <td>0.024028</td>\n",
       "      <td>-0.020115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.130713</td>\n",
       "      <td>0.010443</td>\n",
       "      <td>-0.014208</td>\n",
       "      <td>-0.002364</td>\n",
       "      <td>0.196260</td>\n",
       "      <td>0.054418</td>\n",
       "      <td>0.076723</td>\n",
       "      <td>0.072653</td>\n",
       "      <td>0.075415</td>\n",
       "      <td>0.006812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053816</td>\n",
       "      <td>-0.024634</td>\n",
       "      <td>0.022065</td>\n",
       "      <td>0.329603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.065449</td>\n",
       "      <td>-0.136788</td>\n",
       "      <td>0.147827</td>\n",
       "      <td>-0.019770</td>\n",
       "      <td>0.007675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.008544</td>\n",
       "      <td>-0.074608</td>\n",
       "      <td>0.014277</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.010501</td>\n",
       "      <td>0.080529</td>\n",
       "      <td>-0.022431</td>\n",
       "      <td>-0.071352</td>\n",
       "      <td>-0.070562</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038777</td>\n",
       "      <td>-0.079512</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>-0.008413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>-0.066996</td>\n",
       "      <td>-0.079552</td>\n",
       "      <td>-0.118700</td>\n",
       "      <td>-0.035569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.089284</td>\n",
       "      <td>-0.006820</td>\n",
       "      <td>-0.010213</td>\n",
       "      <td>-0.010742</td>\n",
       "      <td>0.022065</td>\n",
       "      <td>-0.109620</td>\n",
       "      <td>0.012578</td>\n",
       "      <td>-0.032682</td>\n",
       "      <td>0.052964</td>\n",
       "      <td>-0.060491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033923</td>\n",
       "      <td>-0.057615</td>\n",
       "      <td>0.025241</td>\n",
       "      <td>-0.005303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.079663</td>\n",
       "      <td>-0.008066</td>\n",
       "      <td>-0.030277</td>\n",
       "      <td>-0.028413</td>\n",
       "      <td>0.032097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.056860</td>\n",
       "      <td>-0.073835</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>0.165412</td>\n",
       "      <td>0.005726</td>\n",
       "      <td>0.091543</td>\n",
       "      <td>0.132141</td>\n",
       "      <td>-0.010348</td>\n",
       "      <td>-0.064163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033923</td>\n",
       "      <td>0.016373</td>\n",
       "      <td>-0.019755</td>\n",
       "      <td>-0.005303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.050857</td>\n",
       "      <td>-0.087627</td>\n",
       "      <td>0.117004</td>\n",
       "      <td>0.015253</td>\n",
       "      <td>0.032097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.031761</td>\n",
       "      <td>-0.104651</td>\n",
       "      <td>0.006520</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.243685</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>0.083706</td>\n",
       "      <td>0.200566</td>\n",
       "      <td>0.014635</td>\n",
       "      <td>0.045439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016548</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.038137</td>\n",
       "      <td>-0.007516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.038059</td>\n",
       "      <td>-0.119225</td>\n",
       "      <td>0.159007</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.083056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.039709</td>\n",
       "      <td>-0.139389</td>\n",
       "      <td>-0.013874</td>\n",
       "      <td>0.086654</td>\n",
       "      <td>0.093043</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>-0.061630</td>\n",
       "      <td>0.005486</td>\n",
       "      <td>-0.000912</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.498626</td>\n",
       "      <td>0.073974</td>\n",
       "      <td>0.100968</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.098454</td>\n",
       "      <td>-0.102425</td>\n",
       "      <td>0.006019</td>\n",
       "      <td>-0.063413</td>\n",
       "      <td>-0.003612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.206806</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>-0.064018</td>\n",
       "      <td>0.131662</td>\n",
       "      <td>0.261717</td>\n",
       "      <td>-0.003799</td>\n",
       "      <td>0.253075</td>\n",
       "      <td>0.151196</td>\n",
       "      <td>-0.044184</td>\n",
       "      <td>0.306674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074808</td>\n",
       "      <td>0.225314</td>\n",
       "      <td>0.297373</td>\n",
       "      <td>-0.011813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>-0.140302</td>\n",
       "      <td>0.528282</td>\n",
       "      <td>0.255147</td>\n",
       "      <td>0.048367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.130212</td>\n",
       "      <td>-0.117475</td>\n",
       "      <td>-0.010917</td>\n",
       "      <td>-0.055522</td>\n",
       "      <td>0.292268</td>\n",
       "      <td>-0.027114</td>\n",
       "      <td>0.055224</td>\n",
       "      <td>0.160626</td>\n",
       "      <td>0.089904</td>\n",
       "      <td>-0.433352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059455</td>\n",
       "      <td>-0.140348</td>\n",
       "      <td>-0.404699</td>\n",
       "      <td>-0.016526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.053551</td>\n",
       "      <td>0.017808</td>\n",
       "      <td>-0.319503</td>\n",
       "      <td>-0.209877</td>\n",
       "      <td>0.183083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.010115</td>\n",
       "      <td>-0.086336</td>\n",
       "      <td>0.236420</td>\n",
       "      <td>-0.076623</td>\n",
       "      <td>-0.001884</td>\n",
       "      <td>0.043003</td>\n",
       "      <td>-0.074909</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.076582</td>\n",
       "      <td>-0.046084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013426</td>\n",
       "      <td>-0.086819</td>\n",
       "      <td>-0.065142</td>\n",
       "      <td>0.103260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093840</td>\n",
       "      <td>-0.021102</td>\n",
       "      <td>-0.068686</td>\n",
       "      <td>-0.061535</td>\n",
       "      <td>-0.010505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>-0.004343</td>\n",
       "      <td>0.159760</td>\n",
       "      <td>-0.024922</td>\n",
       "      <td>-0.085216</td>\n",
       "      <td>-0.049034</td>\n",
       "      <td>-0.008890</td>\n",
       "      <td>-0.009843</td>\n",
       "      <td>-0.160252</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>-0.089855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259391</td>\n",
       "      <td>0.137201</td>\n",
       "      <td>-0.073978</td>\n",
       "      <td>0.013475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.064603</td>\n",
       "      <td>0.103293</td>\n",
       "      <td>0.096905</td>\n",
       "      <td>0.148357</td>\n",
       "      <td>-0.007401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>-0.169782</td>\n",
       "      <td>-0.230794</td>\n",
       "      <td>0.124802</td>\n",
       "      <td>-0.105332</td>\n",
       "      <td>0.037831</td>\n",
       "      <td>-0.025754</td>\n",
       "      <td>-0.009498</td>\n",
       "      <td>-0.050969</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>0.204646</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.266167</td>\n",
       "      <td>0.510709</td>\n",
       "      <td>0.055459</td>\n",
       "      <td>-0.047183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007338</td>\n",
       "      <td>0.136956</td>\n",
       "      <td>0.287746</td>\n",
       "      <td>0.299608</td>\n",
       "      <td>0.095331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.274211</td>\n",
       "      <td>-0.063062</td>\n",
       "      <td>0.050598</td>\n",
       "      <td>-0.377500</td>\n",
       "      <td>0.062043</td>\n",
       "      <td>-0.022106</td>\n",
       "      <td>-0.149405</td>\n",
       "      <td>-0.008882</td>\n",
       "      <td>0.094392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076970</td>\n",
       "      <td>-0.058274</td>\n",
       "      <td>0.467987</td>\n",
       "      <td>-0.013005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.093668</td>\n",
       "      <td>0.136022</td>\n",
       "      <td>0.182355</td>\n",
       "      <td>0.204642</td>\n",
       "      <td>-0.150610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>-0.192470</td>\n",
       "      <td>-0.039792</td>\n",
       "      <td>-0.027573</td>\n",
       "      <td>-0.106569</td>\n",
       "      <td>-0.045498</td>\n",
       "      <td>-0.054311</td>\n",
       "      <td>-0.080653</td>\n",
       "      <td>-0.050527</td>\n",
       "      <td>-0.098408</td>\n",
       "      <td>0.153896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049994</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>0.052885</td>\n",
       "      <td>0.054068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.093043</td>\n",
       "      <td>-0.030108</td>\n",
       "      <td>0.042828</td>\n",
       "      <td>0.005184</td>\n",
       "      <td>0.030429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>-0.014218</td>\n",
       "      <td>0.030414</td>\n",
       "      <td>-0.001231</td>\n",
       "      <td>0.023161</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>-0.024358</td>\n",
       "      <td>-0.064180</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>-0.002215</td>\n",
       "      <td>-0.029544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031653</td>\n",
       "      <td>-0.012381</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.009749</td>\n",
       "      <td>-0.047060</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>-0.031406</td>\n",
       "      <td>-0.047340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>-0.048532</td>\n",
       "      <td>-0.035451</td>\n",
       "      <td>0.046644</td>\n",
       "      <td>-0.072899</td>\n",
       "      <td>-0.057896</td>\n",
       "      <td>0.041184</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>-0.003367</td>\n",
       "      <td>0.154739</td>\n",
       "      <td>0.102102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147928</td>\n",
       "      <td>0.006046</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>-0.005805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.727015</td>\n",
       "      <td>-0.010316</td>\n",
       "      <td>-0.002547</td>\n",
       "      <td>-0.005308</td>\n",
       "      <td>-0.086873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>-0.154652</td>\n",
       "      <td>-0.125944</td>\n",
       "      <td>0.051123</td>\n",
       "      <td>-0.047675</td>\n",
       "      <td>0.088294</td>\n",
       "      <td>0.013808</td>\n",
       "      <td>0.075032</td>\n",
       "      <td>0.075201</td>\n",
       "      <td>-0.048473</td>\n",
       "      <td>0.068330</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079007</td>\n",
       "      <td>0.117444</td>\n",
       "      <td>0.152438</td>\n",
       "      <td>0.010661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.189453</td>\n",
       "      <td>0.501314</td>\n",
       "      <td>0.175287</td>\n",
       "      <td>0.516590</td>\n",
       "      <td>0.108923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>-0.154598</td>\n",
       "      <td>0.084277</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>-0.052980</td>\n",
       "      <td>-0.358333</td>\n",
       "      <td>0.011963</td>\n",
       "      <td>-0.072962</td>\n",
       "      <td>-0.218323</td>\n",
       "      <td>-0.016159</td>\n",
       "      <td>0.223323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045497</td>\n",
       "      <td>0.388634</td>\n",
       "      <td>0.358961</td>\n",
       "      <td>-0.012150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.065948</td>\n",
       "      <td>0.246310</td>\n",
       "      <td>0.367786</td>\n",
       "      <td>0.431126</td>\n",
       "      <td>-0.083396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>-0.217505</td>\n",
       "      <td>-0.039045</td>\n",
       "      <td>0.022031</td>\n",
       "      <td>-0.064111</td>\n",
       "      <td>-0.139293</td>\n",
       "      <td>0.005911</td>\n",
       "      <td>0.040346</td>\n",
       "      <td>-0.031375</td>\n",
       "      <td>-0.056822</td>\n",
       "      <td>0.199613</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089758</td>\n",
       "      <td>0.372740</td>\n",
       "      <td>0.371309</td>\n",
       "      <td>-0.003645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.182085</td>\n",
       "      <td>0.506839</td>\n",
       "      <td>0.418380</td>\n",
       "      <td>0.691594</td>\n",
       "      <td>0.034138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>-0.219259</td>\n",
       "      <td>-0.049708</td>\n",
       "      <td>0.120165</td>\n",
       "      <td>-0.101982</td>\n",
       "      <td>-0.023104</td>\n",
       "      <td>0.122850</td>\n",
       "      <td>-0.087775</td>\n",
       "      <td>-0.016192</td>\n",
       "      <td>0.056260</td>\n",
       "      <td>-0.086307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032726</td>\n",
       "      <td>-0.392319</td>\n",
       "      <td>-0.204171</td>\n",
       "      <td>0.032357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.040094</td>\n",
       "      <td>0.463171</td>\n",
       "      <td>-0.449177</td>\n",
       "      <td>0.053849</td>\n",
       "      <td>0.020487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.057003</td>\n",
       "      <td>0.194834</td>\n",
       "      <td>-0.040735</td>\n",
       "      <td>-0.014402</td>\n",
       "      <td>-0.127433</td>\n",
       "      <td>-0.051190</td>\n",
       "      <td>0.032453</td>\n",
       "      <td>-0.076897</td>\n",
       "      <td>-0.033862</td>\n",
       "      <td>-0.044350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747330</td>\n",
       "      <td>0.029036</td>\n",
       "      <td>-0.010610</td>\n",
       "      <td>0.022243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.093102</td>\n",
       "      <td>0.149175</td>\n",
       "      <td>0.118731</td>\n",
       "      <td>0.203613</td>\n",
       "      <td>-0.162153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>-0.177121</td>\n",
       "      <td>-0.203435</td>\n",
       "      <td>0.050327</td>\n",
       "      <td>-0.077160</td>\n",
       "      <td>0.070543</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>0.068302</td>\n",
       "      <td>0.024399</td>\n",
       "      <td>-0.015215</td>\n",
       "      <td>0.254764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.272623</td>\n",
       "      <td>0.879162</td>\n",
       "      <td>-0.011459</td>\n",
       "      <td>-0.066412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.042415</td>\n",
       "      <td>0.105693</td>\n",
       "      <td>0.508926</td>\n",
       "      <td>0.448430</td>\n",
       "      <td>0.079669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.030463</td>\n",
       "      <td>0.215832</td>\n",
       "      <td>-0.098979</td>\n",
       "      <td>0.013548</td>\n",
       "      <td>-0.279070</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>0.062253</td>\n",
       "      <td>-0.117925</td>\n",
       "      <td>-0.069844</td>\n",
       "      <td>0.228884</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081426</td>\n",
       "      <td>-0.052481</td>\n",
       "      <td>0.810099</td>\n",
       "      <td>-0.043866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.123366</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.445921</td>\n",
       "      <td>0.297362</td>\n",
       "      <td>-0.068410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.080926</td>\n",
       "      <td>0.048631</td>\n",
       "      <td>-0.009260</td>\n",
       "      <td>-0.021068</td>\n",
       "      <td>0.214977</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.092945</td>\n",
       "      <td>0.096299</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.023395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.032085</td>\n",
       "      <td>0.032931</td>\n",
       "      <td>0.027217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.053710</td>\n",
       "      <td>-0.162693</td>\n",
       "      <td>0.235625</td>\n",
       "      <td>0.031749</td>\n",
       "      <td>0.053808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>-0.049873</td>\n",
       "      <td>0.004522</td>\n",
       "      <td>0.035679</td>\n",
       "      <td>-0.064991</td>\n",
       "      <td>-0.081210</td>\n",
       "      <td>0.078154</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>0.214108</td>\n",
       "      <td>0.076263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115926</td>\n",
       "      <td>-0.011274</td>\n",
       "      <td>-0.092968</td>\n",
       "      <td>-0.016002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.898566</td>\n",
       "      <td>-0.013811</td>\n",
       "      <td>-0.073057</td>\n",
       "      <td>-0.046688</td>\n",
       "      <td>-0.081166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>-0.256627</td>\n",
       "      <td>-0.027394</td>\n",
       "      <td>0.006517</td>\n",
       "      <td>-0.126629</td>\n",
       "      <td>-0.055768</td>\n",
       "      <td>0.067850</td>\n",
       "      <td>0.015444</td>\n",
       "      <td>-0.043371</td>\n",
       "      <td>-0.013899</td>\n",
       "      <td>0.092021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015869</td>\n",
       "      <td>0.096861</td>\n",
       "      <td>0.032699</td>\n",
       "      <td>-0.008024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.121979</td>\n",
       "      <td>0.864417</td>\n",
       "      <td>0.003682</td>\n",
       "      <td>0.676655</td>\n",
       "      <td>0.062174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>-0.072094</td>\n",
       "      <td>0.021588</td>\n",
       "      <td>-0.034349</td>\n",
       "      <td>-0.017201</td>\n",
       "      <td>-0.077276</td>\n",
       "      <td>-0.033589</td>\n",
       "      <td>0.131198</td>\n",
       "      <td>-0.032750</td>\n",
       "      <td>-0.052589</td>\n",
       "      <td>0.307150</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007387</td>\n",
       "      <td>0.665484</td>\n",
       "      <td>0.474834</td>\n",
       "      <td>-0.044152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.129309</td>\n",
       "      <td>0.026729</td>\n",
       "      <td>0.813504</td>\n",
       "      <td>0.583441</td>\n",
       "      <td>-0.010580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>-0.227229</td>\n",
       "      <td>-0.024504</td>\n",
       "      <td>-0.042376</td>\n",
       "      <td>-0.091399</td>\n",
       "      <td>-0.061721</td>\n",
       "      <td>0.011672</td>\n",
       "      <td>0.128242</td>\n",
       "      <td>0.005624</td>\n",
       "      <td>-0.046543</td>\n",
       "      <td>0.258024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018052</td>\n",
       "      <td>0.511562</td>\n",
       "      <td>0.336019</td>\n",
       "      <td>-0.033059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.158144</td>\n",
       "      <td>0.610243</td>\n",
       "      <td>0.549813</td>\n",
       "      <td>0.881998</td>\n",
       "      <td>0.052617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>-0.219194</td>\n",
       "      <td>-0.013236</td>\n",
       "      <td>0.130048</td>\n",
       "      <td>-0.124160</td>\n",
       "      <td>-0.258353</td>\n",
       "      <td>0.108287</td>\n",
       "      <td>-0.191445</td>\n",
       "      <td>-0.182234</td>\n",
       "      <td>0.046601</td>\n",
       "      <td>-0.093831</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009015</td>\n",
       "      <td>-0.377066</td>\n",
       "      <td>-0.263369</td>\n",
       "      <td>0.042103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033788</td>\n",
       "      <td>0.540750</td>\n",
       "      <td>-0.580298</td>\n",
       "      <td>0.011432</td>\n",
       "      <td>-0.062129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.164816</td>\n",
       "      <td>0.230938</td>\n",
       "      <td>-0.018876</td>\n",
       "      <td>0.050471</td>\n",
       "      <td>-0.198941</td>\n",
       "      <td>-0.005347</td>\n",
       "      <td>0.035215</td>\n",
       "      <td>-0.068035</td>\n",
       "      <td>-0.010142</td>\n",
       "      <td>-0.179725</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.192113</td>\n",
       "      <td>-0.077984</td>\n",
       "      <td>0.040477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.085446</td>\n",
       "      <td>0.116959</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.082538</td>\n",
       "      <td>-0.164321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>-0.158009</td>\n",
       "      <td>-0.042638</td>\n",
       "      <td>-0.073439</td>\n",
       "      <td>-0.026926</td>\n",
       "      <td>0.091384</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>0.121349</td>\n",
       "      <td>0.070535</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>0.292155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.192113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016345</td>\n",
       "      <td>-0.090275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.008013</td>\n",
       "      <td>0.093749</td>\n",
       "      <td>0.671383</td>\n",
       "      <td>0.560374</td>\n",
       "      <td>0.036188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.082376</td>\n",
       "      <td>0.092879</td>\n",
       "      <td>-0.091361</td>\n",
       "      <td>0.051946</td>\n",
       "      <td>-0.228688</td>\n",
       "      <td>-0.074180</td>\n",
       "      <td>0.116718</td>\n",
       "      <td>-0.046868</td>\n",
       "      <td>-0.091258</td>\n",
       "      <td>0.295283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077984</td>\n",
       "      <td>0.016345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.092069</td>\n",
       "      <td>-0.072408</td>\n",
       "      <td>0.562825</td>\n",
       "      <td>0.341484</td>\n",
       "      <td>-0.071556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0.090413</td>\n",
       "      <td>0.027401</td>\n",
       "      <td>-0.002545</td>\n",
       "      <td>0.047448</td>\n",
       "      <td>-0.013697</td>\n",
       "      <td>0.068594</td>\n",
       "      <td>-0.002351</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>0.096772</td>\n",
       "      <td>-0.071495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040477</td>\n",
       "      <td>-0.090275</td>\n",
       "      <td>-0.011419</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.019946</td>\n",
       "      <td>-0.017690</td>\n",
       "      <td>-0.051131</td>\n",
       "      <td>-0.049667</td>\n",
       "      <td>-0.036992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>-0.037877</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.067670</td>\n",
       "      <td>-0.046278</td>\n",
       "      <td>-0.065596</td>\n",
       "      <td>0.143272</td>\n",
       "      <td>-0.034971</td>\n",
       "      <td>0.049175</td>\n",
       "      <td>0.248092</td>\n",
       "      <td>0.082009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085446</td>\n",
       "      <td>-0.008013</td>\n",
       "      <td>-0.092069</td>\n",
       "      <td>-0.019946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.043307</td>\n",
       "      <td>-0.069852</td>\n",
       "      <td>-0.064171</td>\n",
       "      <td>-0.086427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-0.271504</td>\n",
       "      <td>0.065684</td>\n",
       "      <td>-0.008471</td>\n",
       "      <td>-0.144721</td>\n",
       "      <td>-0.222170</td>\n",
       "      <td>0.061160</td>\n",
       "      <td>-0.038884</td>\n",
       "      <td>-0.184846</td>\n",
       "      <td>0.016054</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116959</td>\n",
       "      <td>0.093749</td>\n",
       "      <td>-0.072408</td>\n",
       "      <td>-0.017690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.043307</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.124004</td>\n",
       "      <td>0.687877</td>\n",
       "      <td>-0.030798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.018043</td>\n",
       "      <td>0.031688</td>\n",
       "      <td>-0.090370</td>\n",
       "      <td>0.062285</td>\n",
       "      <td>0.129796</td>\n",
       "      <td>-0.027691</td>\n",
       "      <td>0.256200</td>\n",
       "      <td>0.130202</td>\n",
       "      <td>-0.016365</td>\n",
       "      <td>0.297780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.671383</td>\n",
       "      <td>0.562825</td>\n",
       "      <td>-0.051131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.069852</td>\n",
       "      <td>-0.124004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.611219</td>\n",
       "      <td>0.003476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>-0.199728</td>\n",
       "      <td>0.046605</td>\n",
       "      <td>-0.092235</td>\n",
       "      <td>-0.050682</td>\n",
       "      <td>-0.082791</td>\n",
       "      <td>0.021048</td>\n",
       "      <td>0.150979</td>\n",
       "      <td>-0.014430</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>0.255203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082538</td>\n",
       "      <td>0.560374</td>\n",
       "      <td>0.341484</td>\n",
       "      <td>-0.049667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.064171</td>\n",
       "      <td>0.687877</td>\n",
       "      <td>0.611219</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>-0.092381</td>\n",
       "      <td>-0.178080</td>\n",
       "      <td>0.006648</td>\n",
       "      <td>-0.090151</td>\n",
       "      <td>0.323879</td>\n",
       "      <td>-0.099954</td>\n",
       "      <td>0.028305</td>\n",
       "      <td>0.097625</td>\n",
       "      <td>-0.122003</td>\n",
       "      <td>0.019585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164321</td>\n",
       "      <td>0.036188</td>\n",
       "      <td>-0.071556</td>\n",
       "      <td>-0.036992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.086427</td>\n",
       "      <td>-0.030798</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>-0.009275</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279 rows Ã— 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    1.000000 -0.059042 -0.109458  0.381555 -0.004032  0.041149  0.195691   \n",
       "1   -0.059042  1.000000 -0.124685 -0.248104 -0.337101 -0.046771  0.072052   \n",
       "2   -0.109458 -0.124685  1.000000 -0.074957 -0.006329  0.013601 -0.237314   \n",
       "3    0.381555 -0.248104 -0.074957  1.000000  0.100094  0.119826  0.118657   \n",
       "4   -0.004032 -0.337101 -0.006329  0.100094  1.000000  0.021831  0.218681   \n",
       "5    0.041149 -0.046771  0.013601  0.119826  0.021831  1.000000  0.079371   \n",
       "6    0.195691  0.072052 -0.237314  0.118657  0.218681  0.079371  1.000000   \n",
       "7    0.025654 -0.184736 -0.038411  0.149987  0.397435  0.074618  0.166711   \n",
       "8    0.099755 -0.081051  0.029025  0.120668  0.049682  0.670865  0.063044   \n",
       "9   -0.265868  0.069434  0.061539 -0.173355 -0.146043 -0.012412 -0.031786   \n",
       "10   0.008095 -0.141667  0.000108 -0.046153  0.031114  0.114645  0.159971   \n",
       "11  -0.046294  0.013971 -0.106477 -0.026709  0.040207  0.008069 -0.091543   \n",
       "12  -0.285805  0.030865  0.024969 -0.212601 -0.075626  0.020685 -0.084250   \n",
       "13  -0.192239  0.062729  0.286553 -0.174988 -0.006001 -0.045091 -0.654681   \n",
       "14  -0.016675 -0.129383 -0.001940  0.038332  0.118847  0.017437  0.018153   \n",
       "15   0.199533  0.016725 -0.091314  0.117731  0.310412 -0.067438  0.296504   \n",
       "16  -0.120405 -0.149390  0.040893 -0.023805  0.185647  0.053189 -0.084368   \n",
       "17   0.028642 -0.020588  0.006688  0.119989  0.051225  0.012168  0.014332   \n",
       "18        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "19   0.152401 -0.088119 -0.082112  0.145418  0.415102 -0.034179  0.233446   \n",
       "20   0.044455  0.042517  0.004834  0.118853  0.024790 -0.163104 -0.103370   \n",
       "21   0.130713  0.010443 -0.014208 -0.002364  0.196260  0.054418  0.076723   \n",
       "22   0.008544 -0.074608  0.014277  0.001466  0.000549  0.010501  0.080529   \n",
       "23   0.089284 -0.006820 -0.010213 -0.010742  0.022065 -0.109620  0.012578   \n",
       "24   0.056860 -0.073835  0.003253  0.009372  0.165412  0.005726  0.091543   \n",
       "25   0.031761 -0.104651  0.006520  0.070300  0.243685  0.004952  0.083706   \n",
       "26  -0.039709 -0.139389 -0.013874  0.086654  0.093043  0.000195 -0.061630   \n",
       "27   0.206806  0.016074 -0.064018  0.131662  0.261717 -0.003799  0.253075   \n",
       "28  -0.130212 -0.117475 -0.010917 -0.055522  0.292268 -0.027114  0.055224   \n",
       "29  -0.010115 -0.086336  0.236420 -0.076623 -0.001884  0.043003 -0.074909   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "249 -0.004343  0.159760 -0.024922 -0.085216 -0.049034 -0.008890 -0.009843   \n",
       "250 -0.169782 -0.230794  0.124802 -0.105332  0.037831 -0.025754 -0.009498   \n",
       "251  0.018408  0.274211 -0.063062  0.050598 -0.377500  0.062043 -0.022106   \n",
       "252 -0.192470 -0.039792 -0.027573 -0.106569 -0.045498 -0.054311 -0.080653   \n",
       "253 -0.014218  0.030414 -0.001231  0.023161  0.005601 -0.024358 -0.064180   \n",
       "254 -0.048532 -0.035451  0.046644 -0.072899 -0.057896  0.041184  0.013333   \n",
       "255 -0.154652 -0.125944  0.051123 -0.047675  0.088294  0.013808  0.075032   \n",
       "256 -0.154598  0.084277  0.012468 -0.052980 -0.358333  0.011963 -0.072962   \n",
       "257 -0.217505 -0.039045  0.022031 -0.064111 -0.139293  0.005911  0.040346   \n",
       "258 -0.219259 -0.049708  0.120165 -0.101982 -0.023104  0.122850 -0.087775   \n",
       "259  0.057003  0.194834 -0.040735 -0.014402 -0.127433 -0.051190  0.032453   \n",
       "260 -0.177121 -0.203435  0.050327 -0.077160  0.070543 -0.001358  0.068302   \n",
       "261  0.030463  0.215832 -0.098979  0.013548 -0.279070 -0.042551  0.062253   \n",
       "262  0.080926  0.048631 -0.009260 -0.021068  0.214977  0.000599  0.092945   \n",
       "263       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "264 -0.049873  0.004522  0.035679 -0.064991 -0.081210  0.078154 -0.020538   \n",
       "265 -0.256627 -0.027394  0.006517 -0.126629 -0.055768  0.067850  0.015444   \n",
       "266 -0.072094  0.021588 -0.034349 -0.017201 -0.077276 -0.033589  0.131198   \n",
       "267 -0.227229 -0.024504 -0.042376 -0.091399 -0.061721  0.011672  0.128242   \n",
       "268 -0.219194 -0.013236  0.130048 -0.124160 -0.258353  0.108287 -0.191445   \n",
       "269  0.164816  0.230938 -0.018876  0.050471 -0.198941 -0.005347  0.035215   \n",
       "270 -0.158009 -0.042638 -0.073439 -0.026926  0.091384  0.003411  0.121349   \n",
       "271  0.082376  0.092879 -0.091361  0.051946 -0.228688 -0.074180  0.116718   \n",
       "272  0.090413  0.027401 -0.002545  0.047448 -0.013697  0.068594 -0.002351   \n",
       "273       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "274 -0.037877  0.014210  0.067670 -0.046278 -0.065596  0.143272 -0.034971   \n",
       "275 -0.271504  0.065684 -0.008471 -0.144721 -0.222170  0.061160 -0.038884   \n",
       "276  0.018043  0.031688 -0.090370  0.062285  0.129796 -0.027691  0.256200   \n",
       "277 -0.199728  0.046605 -0.092235 -0.050682 -0.082791  0.021048  0.150979   \n",
       "278 -0.092381 -0.178080  0.006648 -0.090151  0.323879 -0.099954  0.028305   \n",
       "\n",
       "          7         8         9      ...          269       270       271  \\\n",
       "0    0.025654  0.099755 -0.265868    ...     0.164816 -0.158009  0.082376   \n",
       "1   -0.184736 -0.081051  0.069434    ...     0.230938 -0.042638  0.092879   \n",
       "2   -0.038411  0.029025  0.061539    ...    -0.018876 -0.073439 -0.091361   \n",
       "3    0.149987  0.120668 -0.173355    ...     0.050471 -0.026926  0.051946   \n",
       "4    0.397435  0.049682 -0.146043    ...    -0.198941  0.091384 -0.228688   \n",
       "5    0.074618  0.670865 -0.012412    ...    -0.005347  0.003411 -0.074180   \n",
       "6    0.166711  0.063044 -0.031786    ...     0.035215  0.121349  0.116718   \n",
       "7    1.000000  0.060302 -0.099283    ...    -0.068035  0.070535 -0.046868   \n",
       "8    0.060302  1.000000 -0.062760    ...    -0.010142  0.007886 -0.091258   \n",
       "9   -0.099283 -0.062760  1.000000    ...    -0.179725  0.292155  0.295283   \n",
       "10   0.040838  0.143195 -0.051925    ...    -0.034564 -0.052905 -0.055454   \n",
       "11  -0.111296  0.054635  0.054649    ...    -0.007202 -0.065499  0.063194   \n",
       "12  -0.107960 -0.073149  0.709528    ...    -0.222315  0.169013  0.132601   \n",
       "13   0.016251  0.043433  0.011351    ...    -0.051391 -0.118857 -0.236425   \n",
       "14   0.024361  0.072590 -0.055314    ...    -0.342547 -0.118037 -0.028179   \n",
       "15   0.251603 -0.025343 -0.271659    ...     0.195220  0.126898  0.199933   \n",
       "16  -0.037765  0.035735  0.213609    ...    -0.112477  0.025226 -0.302136   \n",
       "17   0.013292 -0.011175  0.055497    ...    -0.037438  0.034874  0.040968   \n",
       "18        NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
       "19   0.287783  0.008511 -0.258226    ...    -0.053776  0.126330  0.039008   \n",
       "20   0.018588 -0.164281  0.005523    ...     0.023960  0.084864  0.034307   \n",
       "21   0.072653  0.075415  0.006812    ...     0.053816 -0.024634  0.022065   \n",
       "22  -0.022431 -0.071352 -0.070562    ...    -0.038777 -0.079512  0.001972   \n",
       "23  -0.032682  0.052964 -0.060491    ...     0.033923 -0.057615  0.025241   \n",
       "24   0.132141 -0.010348 -0.064163    ...     0.033923  0.016373 -0.019755   \n",
       "25   0.200566  0.014635  0.045439    ...    -0.016548  0.019802  0.038137   \n",
       "26   0.005486 -0.000912  0.126140    ...    -0.498626  0.073974  0.100968   \n",
       "27   0.151196 -0.044184  0.306674    ...     0.074808  0.225314  0.297373   \n",
       "28   0.160626  0.089904 -0.433352    ...     0.059455 -0.140348 -0.404699   \n",
       "29   0.001327  0.076582 -0.046084    ...     0.013426 -0.086819 -0.065142   \n",
       "..        ...       ...       ...    ...          ...       ...       ...   \n",
       "249 -0.160252 -0.000557 -0.089855    ...     0.259391  0.137201 -0.073978   \n",
       "250 -0.050969 -0.001121  0.204646    ...    -0.266167  0.510709  0.055459   \n",
       "251 -0.149405 -0.008882  0.094392    ...     0.076970 -0.058274  0.467987   \n",
       "252 -0.050527 -0.098408  0.153896    ...    -0.049994  0.058780  0.052885   \n",
       "253  0.012682 -0.002215 -0.029544    ...    -0.031653 -0.012381  0.002524   \n",
       "254 -0.003367  0.154739  0.102102    ...    -0.147928  0.006046  0.025644   \n",
       "255  0.075201 -0.048473  0.068330    ...    -0.079007  0.117444  0.152438   \n",
       "256 -0.218323 -0.016159  0.223323    ...    -0.045497  0.388634  0.358961   \n",
       "257 -0.031375 -0.056822  0.199613    ...    -0.089758  0.372740  0.371309   \n",
       "258 -0.016192  0.056260 -0.086307    ...    -0.032726 -0.392319 -0.204171   \n",
       "259 -0.076897 -0.033862 -0.044350    ...     0.747330  0.029036 -0.010610   \n",
       "260  0.024399 -0.015215  0.254764    ...    -0.272623  0.879162 -0.011459   \n",
       "261 -0.117925 -0.069844  0.228884    ...    -0.081426 -0.052481  0.810099   \n",
       "262  0.096299  0.022300  0.023395    ...     0.023430  0.032085  0.032931   \n",
       "263       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
       "264 -0.001012  0.214108  0.076263    ...    -0.115926 -0.011274 -0.092968   \n",
       "265 -0.043371 -0.013899  0.092021    ...    -0.015869  0.096861  0.032699   \n",
       "266 -0.032750 -0.052589  0.307150    ...    -0.007387  0.665484  0.474834   \n",
       "267  0.005624 -0.046543  0.258024    ...    -0.018052  0.511562  0.336019   \n",
       "268 -0.182234  0.046601 -0.093831    ...    -0.009015 -0.377066 -0.263369   \n",
       "269 -0.068035 -0.010142 -0.179725    ...     1.000000 -0.192113 -0.077984   \n",
       "270  0.070535  0.007886  0.292155    ...    -0.192113  1.000000  0.016345   \n",
       "271 -0.046868 -0.091258  0.295283    ...    -0.077984  0.016345  1.000000   \n",
       "272  0.008556  0.096772 -0.071495    ...     0.040477 -0.090275 -0.011419   \n",
       "273       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
       "274  0.049175  0.248092  0.082009    ...    -0.085446 -0.008013 -0.092069   \n",
       "275 -0.184846  0.016054  0.063300    ...     0.116959  0.093749 -0.072408   \n",
       "276  0.130202 -0.016365  0.297780    ...     0.002630  0.671383  0.562825   \n",
       "277 -0.014430  0.004283  0.255203    ...     0.082538  0.560374  0.341484   \n",
       "278  0.097625 -0.122003  0.019585    ...    -0.164321  0.036188 -0.071556   \n",
       "\n",
       "          272  273       274       275       276       277       278  \n",
       "0    0.090413  NaN -0.037877 -0.271504  0.018043 -0.199728 -0.092381  \n",
       "1    0.027401  NaN  0.014210  0.065684  0.031688  0.046605 -0.178080  \n",
       "2   -0.002545  NaN  0.067670 -0.008471 -0.090370 -0.092235  0.006648  \n",
       "3    0.047448  NaN -0.046278 -0.144721  0.062285 -0.050682 -0.090151  \n",
       "4   -0.013697  NaN -0.065596 -0.222170  0.129796 -0.082791  0.323879  \n",
       "5    0.068594  NaN  0.143272  0.061160 -0.027691  0.021048 -0.099954  \n",
       "6   -0.002351  NaN -0.034971 -0.038884  0.256200  0.150979  0.028305  \n",
       "7    0.008556  NaN  0.049175 -0.184846  0.130202 -0.014430  0.097625  \n",
       "8    0.096772  NaN  0.248092  0.016054 -0.016365  0.004283 -0.122003  \n",
       "9   -0.071495  NaN  0.082009  0.063300  0.297780  0.255203  0.019585  \n",
       "10   0.004383  NaN  0.071090  0.157539 -0.062546  0.084336  0.005178  \n",
       "11  -0.003007  NaN  0.212522  0.039697 -0.036134 -0.013432 -0.012897  \n",
       "12  -0.066043  NaN  0.052213  0.194113  0.101686  0.214848  0.069403  \n",
       "13   0.032815  NaN  0.179483 -0.083605 -0.249300 -0.234168  0.008331  \n",
       "14  -0.042085  NaN  0.086201 -0.037790 -0.170610 -0.163450  0.003965  \n",
       "15   0.094711  NaN -0.103729 -0.241676  0.484570  0.159676  0.042674  \n",
       "16  -0.081102  NaN  0.008272  0.099483 -0.267412 -0.102082  0.195198  \n",
       "17  -0.007184  NaN -0.008735  0.040192  0.060671  0.073527  0.063999  \n",
       "18        NaN  NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "19  -0.007327  NaN -0.003834 -0.198374  0.309029  0.072924  0.042764  \n",
       "20  -0.003746  NaN -0.137654 -0.033795  0.060650  0.024028 -0.020115  \n",
       "21   0.329603  NaN -0.065449 -0.136788  0.147827 -0.019770  0.007675  \n",
       "22  -0.008413  NaN  0.001577 -0.066996 -0.079552 -0.118700 -0.035569  \n",
       "23  -0.005303  NaN -0.079663 -0.008066 -0.030277 -0.028413  0.032097  \n",
       "24  -0.005303  NaN -0.050857 -0.087627  0.117004  0.015253  0.032097  \n",
       "25  -0.007516  NaN -0.038059 -0.119225  0.159007  0.012541  0.083056  \n",
       "26   0.007432  NaN  0.098454 -0.102425  0.006019 -0.063413 -0.003612  \n",
       "27  -0.011813  NaN  0.000868 -0.140302  0.528282  0.255147  0.048367  \n",
       "28  -0.016526  NaN -0.053551  0.017808 -0.319503 -0.209877  0.183083  \n",
       "29   0.103260  NaN  0.093840 -0.021102 -0.068686 -0.061535 -0.010505  \n",
       "..        ...  ...       ...       ...       ...       ...       ...  \n",
       "249  0.013475  NaN -0.064603  0.103293  0.096905  0.148357 -0.007401  \n",
       "250 -0.047183  NaN  0.007338  0.136956  0.287746  0.299608  0.095331  \n",
       "251 -0.013005  NaN -0.093668  0.136022  0.182355  0.204642 -0.150610  \n",
       "252  0.054068  NaN -0.093043 -0.030108  0.042828  0.005184  0.030429  \n",
       "253  0.004948  NaN -0.009749 -0.047060  0.008386 -0.031406 -0.047340  \n",
       "254 -0.005805  NaN  0.727015 -0.010316 -0.002547 -0.005308 -0.086873  \n",
       "255  0.010661  NaN -0.189453  0.501314  0.175287  0.516590  0.108923  \n",
       "256 -0.012150  NaN -0.065948  0.246310  0.367786  0.431126 -0.083396  \n",
       "257 -0.003645  NaN -0.182085  0.506839  0.418380  0.691594  0.034138  \n",
       "258  0.032357  NaN  0.040094  0.463171 -0.449177  0.053849  0.020487  \n",
       "259  0.022243  NaN -0.093102  0.149175  0.118731  0.203613 -0.162153  \n",
       "260 -0.066412  NaN -0.042415  0.105693  0.508926  0.448430  0.079669  \n",
       "261 -0.043866  NaN -0.123366  0.002916  0.445921  0.297362 -0.068410  \n",
       "262  0.027217  NaN -0.053710 -0.162693  0.235625  0.031749  0.053808  \n",
       "263       NaN  NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "264 -0.016002  NaN  0.898566 -0.013811 -0.073057 -0.046688 -0.081166  \n",
       "265 -0.008024  NaN -0.121979  0.864417  0.003682  0.676655  0.062174  \n",
       "266 -0.044152  NaN -0.129309  0.026729  0.813504  0.583441 -0.010580  \n",
       "267 -0.033059  NaN -0.158144  0.610243  0.549813  0.881998  0.052617  \n",
       "268  0.042103  NaN  0.033788  0.540750 -0.580298  0.011432 -0.062129  \n",
       "269  0.040477  NaN -0.085446  0.116959  0.002630  0.082538 -0.164321  \n",
       "270 -0.090275  NaN -0.008013  0.093749  0.671383  0.560374  0.036188  \n",
       "271 -0.011419  NaN -0.092069 -0.072408  0.562825  0.341484 -0.071556  \n",
       "272  1.000000  NaN -0.019946 -0.017690 -0.051131 -0.049667 -0.036992  \n",
       "273       NaN  NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "274 -0.019946  NaN  1.000000 -0.043307 -0.069852 -0.064171 -0.086427  \n",
       "275 -0.017690  NaN -0.043307  1.000000 -0.124004  0.687877 -0.030798  \n",
       "276 -0.051131  NaN -0.069852 -0.124004  1.000000  0.611219  0.003476  \n",
       "277 -0.049667  NaN -0.064171  0.687877  0.611219  1.000000 -0.009275  \n",
       "278 -0.036992  NaN -0.086427 -0.030798  0.003476 -0.009275  1.000000  \n",
       "\n",
       "[279 rows x 279 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_missing.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see there is no major correlation between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X, y split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1      2     3      4      5      6      7      8     9    ...   268  \\\n",
       "0  75.0  0.0  190.0  80.0   91.0  193.0  371.0  174.0  121.0 -16.0  ...  -0.3   \n",
       "1  56.0  1.0  165.0  64.0   81.0  174.0  401.0  149.0   39.0  25.0  ...  -0.5   \n",
       "2  54.0  0.0  172.0  95.0  138.0  163.0  386.0  185.0  102.0  96.0  ...   0.9   \n",
       "3  55.0  0.0  175.0  94.0  100.0  202.0  380.0  179.0  143.0  28.0  ...   0.1   \n",
       "4  75.0  0.0  190.0  80.0   88.0  181.0  360.0  177.0  103.0 -16.0  ...  -0.4   \n",
       "\n",
       "   269   270  271  272  273  274  275   276   277  \n",
       "0  0.0   9.0 -0.9  0.0  0.0  0.9  2.9  23.3  49.4  \n",
       "1  0.0   8.5  0.0  0.0  0.0  0.2  2.1  20.4  38.8  \n",
       "2  0.0   9.5 -2.4  0.0  0.0  0.3  3.4  12.3  49.0  \n",
       "3  0.0  12.2 -2.2  0.0  0.0  0.4  2.6  34.6  61.6  \n",
       "4  0.0  13.1 -3.6  0.0  0.0 -0.1  3.9  25.4  62.8  \n",
       "\n",
       "[5 rows x 278 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_no_missing.drop(columns=278)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_columns=[\"Age\",\"Gender_Nom\",\"Height\",\"Weight\",\"QRS_Dur\",\n",
    "\"P-R_Int\",\"Q-T_Int\",\"T_Int\",\"P_Int\",\"QRS\",\"T\",\"P\",\"J\",\"Heart_Rate\",\n",
    "\"Q_Wave\",\"R_Wave\",\"S_Wave\",\"R_Prime\",\"S_Prime\",\"Int_Def\",\"Rag_R_Nom\",\n",
    "\"Diph_R_Nom\",\"Rag_P_Nom\",\"Diph_P_Nom\",\"Rag_T_Nom\",\"Diph_T_Nom\", \n",
    "\"DII00\", \"DII01\",\"DII02\", \"DII03\", \"DII04\",\"DII05\",\"DII06\",\"DII07\",\"DII08\",\"DII09\",\"DII10\",\"DII11\",\n",
    "\"DIII00\",\"DIII01\",\"DIII02\", \"DIII03\", \"DIII04\",\"DIII05\",\"DIII06\",\"DIII07\",\"DIII08\",\"DIII09\",\"DIII10\",\"DIII11\",\n",
    "\"AVR00\",\"AVR01\",\"AVR02\",\"AVR03\",\"AVR04\",\"AVR05\",\"AVR06\",\"AVR07\",\"AVR08\",\"AVR09\",\"AVR10\",\"AVR11\",\n",
    "\"AVL00\",\"AVRL1\",\"AVL02\",\"AVL03\",\"AVL04\",\"AVL05\",\"AVL06\",\"AVL07\",\"AVL08\",\"AVL09\",\"AVL10\",\"AVL11\",\n",
    "\"AVF00\",\"AVF01\",\"AVF02\",\"AVF03\",\"AVF04\",\"AVF05\",\"AVF06\",\"AVF07\",\"AVF08\",\"AVF09\",\"AVF10\",\"AVF11\",\n",
    "\"V100\",\"V101\",\"V102\",\"V103\",\"V104\",\"V105\",\"V106\",\"V107\",\"V108\",\"V109\",\"V110\",\"V111\",\n",
    "\"V200\",\"V201\",\"V202\",\"V203\",\"V204\",\"V205\",\"V206\",\"V207\",\"V208\",\"V209\",\"V210\",\"V211\",\n",
    "\"V300\",\"V301\",\"V302\",\"V303\",\"V304\",\"V305\",\"V306\",\"V307\",\"V308\",\"V309\",\"V310\",\"V311\",\n",
    "\"V400\",\"V401\",\"V402\",\"V403\",\"V404\",\"V405\",\"V406\",\"V407\",\"V408\",\"V409\",\"V410\",\"V411\",\n",
    "\"V500\",\"V501\",\"V502\",\"V503\",\"V504\",\"V505\",\"V506\",\"V507\",\"V508\",\"V509\",\"V510\",\"V511\",\n",
    "\"V600\",\"V601\",\"V602\",\"V603\",\"V604\",\"V605\",\"V606\",\"V607\",\"V608\",\"V609\",\"V610\",\"V611\",\n",
    "\"JJ_Wave\",\"Q_Wave\",\"R_Wave\",\"S_Wave\",\"R_Prime_Wave\",\"S_Prime_Wave\",\"P_Wave\",\"T_Wave\",\n",
    "\"QRSA\",\"QRSTA\",\"DII170\",\"DII171\",\"DII172\",\"DII173\",\"DII174\",\"DII175\",\"DII176\",\"DII177\",\"DII178\",\"DII179\",\n",
    "\"DIII180\",\"DIII181\",\"DIII182\",\"DIII183\",\"DIII184\",\"DIII185\",\"DIII186\",\"DIII187\",\"DIII188\",\"DIII189\",\n",
    "\"AVR190\",\"AVR191\",\"AVR192\",\"AVR193\",\"AVR194\",\"AVR195\",\"AVR196\",\"AVR197\",\"AVR198\",\"AVR199\",\n",
    "\"AVL200\",\"AVL201\",\"AVL202\",\"AVL203\",\"AVL204\",\"AVL205\",\"AVL206\",\"AVL207\",\"AVL208\",\"AVL209\",\n",
    "\"AVF210\",\"AVF211\",\"AVF212\",\"AVF213\",\"AVF214\",\"AVF215\",\"AVF216\",\"AVF217\",\"AVF218\",\"AVF219\",\n",
    "\"V1220\",\"V1221\",\"V1222\",\"V1223\",\"V1224\",\"V1225\",\"V1226\",\"V1227\",\"V1228\",\"V1229\",\n",
    "\"V2230\",\"V2231\",\"V2232\",\"V2233\",\"V2234\",\"V2235\",\"V2236\",\"V2237\",\"V2238\",\"V2239\",\n",
    "\"V3240\",\"V3241\",\"V3242\",\"V3243\",\"V3244\",\"V3245\",\"V3246\",\"V3247\",\"V3248\",\"V3249\",\n",
    "\"V4250\",\"V4251\",\"V4252\",\"V4253\",\"V4254\",\"V4255\",\"V4256\",\"V4257\",\"V4258\",\"V4259\",\n",
    "\"V5260\",\"V5261\",\"V5262\",\"V5263\",\"V5264\",\"V5265\",\"V5266\",\"V5267\",\"V5268\",\"V5269\",\n",
    "\"V6270\",\"V6271\",\"V6272\",\"V6273\",\"V6274\",\"V6275\",\"V6276\",\"V6277\",\"V6278\",\"V6279\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender_Nom</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>QRS_Dur</th>\n",
       "      <th>P-R_Int</th>\n",
       "      <th>Q-T_Int</th>\n",
       "      <th>T_Int</th>\n",
       "      <th>P_Int</th>\n",
       "      <th>QRS</th>\n",
       "      <th>...</th>\n",
       "      <th>V6270</th>\n",
       "      <th>V6271</th>\n",
       "      <th>V6272</th>\n",
       "      <th>V6273</th>\n",
       "      <th>V6274</th>\n",
       "      <th>V6275</th>\n",
       "      <th>V6276</th>\n",
       "      <th>V6277</th>\n",
       "      <th>V6278</th>\n",
       "      <th>V6279</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Gender_Nom  Height  Weight  QRS_Dur  P-R_Int  Q-T_Int  T_Int  P_Int  \\\n",
       "0  75.0         0.0   190.0    80.0     91.0    193.0    371.0  174.0  121.0   \n",
       "1  56.0         1.0   165.0    64.0     81.0    174.0    401.0  149.0   39.0   \n",
       "2  54.0         0.0   172.0    95.0    138.0    163.0    386.0  185.0  102.0   \n",
       "3  55.0         0.0   175.0    94.0    100.0    202.0    380.0  179.0  143.0   \n",
       "4  75.0         0.0   190.0    80.0     88.0    181.0    360.0  177.0  103.0   \n",
       "\n",
       "    QRS  ...    V6270  V6271  V6272  V6273  V6274  V6275  V6276  V6277  V6278  \\\n",
       "0 -16.0  ...     -0.3    0.0    9.0   -0.9    0.0    0.0    0.9    2.9   23.3   \n",
       "1  25.0  ...     -0.5    0.0    8.5    0.0    0.0    0.0    0.2    2.1   20.4   \n",
       "2  96.0  ...      0.9    0.0    9.5   -2.4    0.0    0.0    0.3    3.4   12.3   \n",
       "3  28.0  ...      0.1    0.0   12.2   -2.2    0.0    0.0    0.4    2.6   34.6   \n",
       "4 -16.0  ...     -0.4    0.0   13.1   -3.6    0.0    0.0   -0.1    3.9   25.4   \n",
       "\n",
       "   V6279  \n",
       "0   49.4  \n",
       "1   38.8  \n",
       "2   49.0  \n",
       "3   61.6  \n",
       "4   62.8  \n",
       "\n",
       "[5 rows x 278 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns = X_columns\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     8.0\n",
       "1     6.0\n",
       "2    10.0\n",
       "3     1.0\n",
       "4     7.0\n",
       "Name: 278, dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data_no_missing[278]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     8.0\n",
       "1     6.0\n",
       "2    10.0\n",
       "3     1.0\n",
       "4     7.0\n",
       "Name: 278, dtype: float64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.columns = [\"Class\"]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>28.5</td>\n",
       "      <td>31.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>24.2</td>\n",
       "      <td>48.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>14.3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.4</td>\n",
       "      <td>24.8</td>\n",
       "      <td>57.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>16.1</td>\n",
       "      <td>23.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1      2     3      4      5      6      7      8     9    ...   \\\n",
       "303  67.0  0.0  176.0  80.0   97.0  144.0  357.0  170.0  100.0 -48.0  ...    \n",
       "131  48.0  1.0  165.0  70.0   83.0  146.0  418.0  134.0   83.0  48.0  ...    \n",
       "116  74.0  0.0  172.0  74.0  106.0  165.0  411.0  161.0   96.0  55.0  ...    \n",
       "305  14.0  0.0  175.0  59.0   96.0  141.0  340.0  225.0   87.0  80.0  ...    \n",
       "65   44.0  1.0  155.0  65.0   80.0  117.0  363.0  142.0   72.0  56.0  ...    \n",
       "\n",
       "     268  269   270  271  272  273  274  275   276   277  \n",
       "303 -0.2  0.0   3.6 -2.6  0.0  0.0  1.0  1.5   0.2  14.6  \n",
       "131 -1.0  0.0  11.9  0.0  0.0  0.0  1.0  0.6  28.5  31.7  \n",
       "116 -0.5 -0.5   6.5  0.0  0.0  0.0  0.3  2.6  24.2  48.6  \n",
       "305  0.1 -0.8  14.3 -2.5  0.0  0.0  0.5  3.4  24.8  57.4  \n",
       "65  -0.2  0.0   6.2  0.0  0.0  0.0  0.5  1.1  16.1  23.8  \n",
       "\n",
       "[5 rows x 278 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_org.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Machine Learning Models (Without Bagging Boosting and PCA)\n",
    "\n",
    "- We will be using \"WEIGHTED RECALL\" as evaluation strategy because we are predicting Cardiac Arrhythmia, which is serious medical condition.\n",
    "- We don't want to misclassify someone having arrhythmia as normal. It is lot bigger risk than classifying someone normal as having arrhythmia. So we want to maximize true positive rate i.e. Recall.\n",
    "- Weighted recall is used instead of normal recall because it accounts for label imbalance present in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN clasiification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [1, 2, 3, 4, 5, 7, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "param_grid={'n_neighbors':[1,2,3,4,5,7,10]}\n",
    "\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00499992, 0.00631833, 0.00419111, 0.0056232 , 0.00600724,\n",
       "        0.00467148, 0.00501127]),\n",
       " 'mean_score_time': array([0.108917  , 0.11035643, 0.10880852, 0.10997972, 0.11043725,\n",
       "        0.10955048, 0.11013665]),\n",
       " 'mean_test_score': array([0.55208333, 0.59114583, 0.59635417, 0.578125  , 0.57291667,\n",
       "        0.5625    , 0.56770833]),\n",
       " 'mean_train_score': array([1.        , 0.68618421, 0.66540019, 0.63084585, 0.62170995,\n",
       "        0.60026961, 0.57163885]),\n",
       " 'param_n_neighbors': masked_array(data=[1, 2, 3, 4, 5, 7, 10],\n",
       "              mask=[False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_neighbors': 1},\n",
       "  {'n_neighbors': 2},\n",
       "  {'n_neighbors': 3},\n",
       "  {'n_neighbors': 4},\n",
       "  {'n_neighbors': 5},\n",
       "  {'n_neighbors': 7},\n",
       "  {'n_neighbors': 10}],\n",
       " 'rank_test_score': array([7, 2, 1, 3, 4, 6, 5]),\n",
       " 'split0_test_score': array([0.57317073, 0.58536585, 0.59756098, 0.54878049, 0.53658537,\n",
       "        0.53658537, 0.52439024]),\n",
       " 'split0_train_score': array([1.        , 0.67218543, 0.65562914, 0.62251656, 0.62251656,\n",
       "        0.60927152, 0.5794702 ]),\n",
       " 'split1_test_score': array([0.5125, 0.5875, 0.6   , 0.575 , 0.5625, 0.5625, 0.5625]),\n",
       " 'split1_train_score': array([1.        , 0.6875    , 0.68092105, 0.62828947, 0.61184211,\n",
       "        0.58552632, 0.56578947]),\n",
       " 'split2_test_score': array([0.51948052, 0.57142857, 0.58441558, 0.54545455, 0.55844156,\n",
       "        0.53246753, 0.55844156]),\n",
       " 'split2_train_score': array([1.        , 0.70684039, 0.67100977, 0.64820847, 0.62540717,\n",
       "        0.60912052, 0.5732899 ]),\n",
       " 'split3_test_score': array([0.57534247, 0.61643836, 0.5890411 , 0.57534247, 0.57534247,\n",
       "        0.57534247, 0.5890411 ]),\n",
       " 'split3_train_score': array([1.        , 0.69131833, 0.66559486, 0.62700965, 0.62057878,\n",
       "        0.59807074, 0.56913183]),\n",
       " 'split4_test_score': array([0.58333333, 0.59722222, 0.61111111, 0.65277778, 0.63888889,\n",
       "        0.61111111, 0.61111111]),\n",
       " 'split4_train_score': array([1.        , 0.67307692, 0.65384615, 0.62820513, 0.62820513,\n",
       "        0.59935897, 0.57051282]),\n",
       " 'std_fit_time': array([0.00244972, 0.00218873, 0.0023433 , 0.00257356, 0.00200125,\n",
       "        0.00267021, 0.00246417]),\n",
       " 'std_score_time': array([0.00252271, 0.00212495, 0.00214259, 0.00183472, 0.00183015,\n",
       "        0.00197403, 0.00210974]),\n",
       " 'std_test_score': array([0.03033932, 0.01471036, 0.00909168, 0.03803103, 0.03411259,\n",
       "        0.02826112, 0.02938195]),\n",
       " 'std_train_score': array([0.        , 0.01282603, 0.01001371, 0.00893379, 0.00557066,\n",
       "        0.00874531, 0.00460053])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 3}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best Parameter : No. of neighbours=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5963541666666666"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf=KNeighborsClassifier(n_neighbors=3)\n",
    "knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.6692708333333334\n",
      "Test Recall score: 0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = knn_clf.predict(X_train)\n",
    "y_pred_test = knn_clf.predict(X_test)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that test score is poor and KNN doesn't perform well.\n",
    "- But model is a good fit as train and test scores are almost same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lreg_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "param_grid = {'C': [0.00001,0.0001,0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "grid_search = GridSearchCV(lreg_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.07710714, 0.07369571, 0.07159019, 0.06918383, 0.07339482,\n",
       "        0.07590184, 0.07319431, 0.07459807]),\n",
       " 'mean_score_time': array([0.00060186, 0.00060177, 0.00070205, 0.0007021 , 0.00060167,\n",
       "        0.00050144, 0.00070229, 0.00060163]),\n",
       " 'mean_test_score': array([0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.609375  ,\n",
       "        0.70052083, 0.68489583, 0.66666667]),\n",
       " 'mean_train_score': array([0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.64196967,\n",
       "        0.83792892, 0.96622393, 0.99481065]),\n",
       " 'param_C': masked_array(data=[1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1e-05},\n",
       "  {'C': 0.0001},\n",
       "  {'C': 0.001},\n",
       "  {'C': 0.01},\n",
       "  {'C': 0.1},\n",
       "  {'C': 1},\n",
       "  {'C': 10},\n",
       "  {'C': 100}],\n",
       " 'rank_test_score': array([5, 5, 5, 5, 4, 1, 2, 3]),\n",
       " 'split0_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.59756098,\n",
       "        0.67073171, 0.69512195, 0.65853659]),\n",
       " 'split0_train_score': array([0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.64238411,\n",
       "        0.8410596 , 0.97019868, 0.99668874]),\n",
       " 'split1_test_score': array([0.5125, 0.5125, 0.5125, 0.5125, 0.6   , 0.7125, 0.6375, 0.6375]),\n",
       " 'split1_train_score': array([0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.64802632,\n",
       "        0.84210526, 0.97039474, 0.99342105]),\n",
       " 'split2_test_score': array([0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.58441558,\n",
       "        0.67532468, 0.63636364, 0.62337662]),\n",
       " 'split2_train_score': array([0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.64495114,\n",
       "        0.83713355, 0.9771987 , 1.        ]),\n",
       " 'split3_test_score': array([0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.61643836,\n",
       "        0.69863014, 0.73972603, 0.7260274 ]),\n",
       " 'split3_train_score': array([0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.63987138,\n",
       "        0.83601286, 0.95819936, 0.9903537 ]),\n",
       " 'split4_test_score': array([0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.65277778,\n",
       "        0.75      , 0.72222222, 0.69444444]),\n",
       " 'split4_train_score': array([0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.63461538,\n",
       "        0.83333333, 0.95512821, 0.99358974]),\n",
       " 'std_fit_time': array([0.00457912, 0.00095118, 0.00188646, 0.00477697, 0.00351058,\n",
       "        0.0014392 , 0.00126829, 0.00120328]),\n",
       " 'std_score_time': array([3.75158209e-04, 2.00629369e-04, 2.45651565e-04, 2.45709908e-04,\n",
       "        2.00319546e-04, 3.50402318e-07, 2.45554220e-04, 2.00462398e-04]),\n",
       " 'std_test_score': array([0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02314586,\n",
       "        0.02839729, 0.04237194, 0.03717082]),\n",
       " 'std_train_score': array([0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00456579,\n",
       "        0.0032455 , 0.00826013, 0.00327849])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameter : C=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7005208333333334"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs',C=1)\n",
    "lreg_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.8411458333333334\n",
      "Test Recall score: 0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = lreg_clf.predict(X_train)\n",
    "y_pred_test = lreg_clf.predict(X_test)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that test score is poor and logistic regression doesn't perform well.\n",
    "- Also model is overfitting as there is large difference between train and test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=None,\n",
       "     tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "LSVC_clf = LinearSVC(multi_class='crammer_singer')\n",
    "\n",
    "param_grid = {'C': [0.00001,0.0001,0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "grid_search = GridSearchCV(LSVC_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00922437, 0.02650638, 0.05444465, 0.10368438, 0.28786979,\n",
       "        0.86402235, 2.53912015, 2.57618418]),\n",
       " 'mean_score_time': array([0.00060177, 0.0007987 , 0.00060139, 0.00088673, 0.00070243,\n",
       "        0.00080266, 0.00088382, 0.00059848]),\n",
       " 'mean_test_score': array([0.53385417, 0.53385417, 0.53385417, 0.578125  , 0.69791667,\n",
       "        0.69791667, 0.609375  , 0.60677083]),\n",
       " 'mean_train_score': array([0.53393884, 0.53393884, 0.53393884, 0.60413872, 0.78261155,\n",
       "        0.94141337, 0.99871795, 1.        ]),\n",
       " 'param_C': masked_array(data=[1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1e-05},\n",
       "  {'C': 0.0001},\n",
       "  {'C': 0.001},\n",
       "  {'C': 0.01},\n",
       "  {'C': 0.1},\n",
       "  {'C': 1},\n",
       "  {'C': 10},\n",
       "  {'C': 100}],\n",
       " 'rank_test_score': array([6, 6, 6, 5, 1, 1, 3, 4]),\n",
       " 'split0_test_score': array([0.5       , 0.5       , 0.5       , 0.53658537, 0.68292683,\n",
       "        0.67073171, 0.62195122, 0.6097561 ]),\n",
       " 'split0_train_score': array([0.54304636, 0.54304636, 0.54304636, 0.59933775, 0.79139073,\n",
       "        0.93708609, 1.        , 1.        ]),\n",
       " 'split1_test_score': array([0.5125, 0.5125, 0.5125, 0.5625, 0.675 , 0.7   , 0.55  , 0.5625]),\n",
       " 'split1_train_score': array([0.53947368, 0.53947368, 0.53947368, 0.60855263, 0.78618421,\n",
       "        0.94078947, 1.        , 1.        ]),\n",
       " 'split2_test_score': array([0.53246753, 0.53246753, 0.53246753, 0.58441558, 0.7012987 ,\n",
       "        0.67532468, 0.5974026 , 0.5974026 ]),\n",
       " 'split2_train_score': array([0.53420195, 0.53420195, 0.53420195, 0.59609121, 0.77850163,\n",
       "        0.95439739, 1.        , 1.        ]),\n",
       " 'split3_test_score': array([0.56164384, 0.56164384, 0.56164384, 0.60273973, 0.69863014,\n",
       "        0.71232877, 0.67123288, 0.67123288]),\n",
       " 'split3_train_score': array([0.52733119, 0.52733119, 0.52733119, 0.61414791, 0.77813505,\n",
       "        0.93569132, 1.        , 1.        ]),\n",
       " 'split4_test_score': array([0.56944444, 0.56944444, 0.56944444, 0.61111111, 0.73611111,\n",
       "        0.73611111, 0.61111111, 0.59722222]),\n",
       " 'split4_train_score': array([0.52564103, 0.52564103, 0.52564103, 0.6025641 , 0.77884615,\n",
       "        0.93910256, 0.99358974, 1.        ]),\n",
       " 'std_fit_time': array([0.00075095, 0.00145987, 0.00121174, 0.01583451, 0.02946165,\n",
       "        0.0746124 , 0.38464936, 0.29439875]),\n",
       " 'std_score_time': array([0.00049134, 0.00024529, 0.00020119, 0.00056843, 0.00024544,\n",
       "        0.0002453 , 0.00020311, 0.00020263]),\n",
       " 'std_test_score': array([0.02691538, 0.02691538, 0.02691538, 0.02732159, 0.02081613,\n",
       "        0.02395988, 0.03905063, 0.03514059]),\n",
       " 'std_train_score': array([0.00672555, 0.00672555, 0.00672555, 0.00648009, 0.00530936,\n",
       "        0.00671984, 0.0025641 , 0.        ])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameter : C=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6979166666666666"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=None,\n",
       "     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSVC_clf = LinearSVC(multi_class='crammer_singer', C=0.1)\n",
    "LSVC_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.7838541666666666\n",
      "Test Recall score: 0.7205882352941176\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = LSVC_clf.predict(X_train)\n",
    "y_pred_test = LSVC_clf.predict(X_test)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that test score is better than KNN and logistic regression, so Linear SVC performs better.\n",
    "- Also model is good fit as there is not much difference between train and test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelised SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 1, 10], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "KSVC_clf = svm.SVC(kernel='rbf')\n",
    "\n",
    "param_grid = {'C': [0.0001,0.001,0.01,0.1,1,10],\n",
    "          'gamma': [0.0001,0.001,0.01,0.1,1,10]}\n",
    "\n",
    "grid_search = GridSearchCV(KSVC_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.03183517, 0.0295867 , 0.02937765, 0.03058062, 0.03047056,\n",
       "        0.02936511, 0.02957821, 0.02918053, 0.02919021, 0.02928953,\n",
       "        0.03056827, 0.03476009, 0.02879028, 0.028476  , 0.03165231,\n",
       "        0.0463388 , 0.08902864, 0.10987959, 0.03118219, 0.03165193,\n",
       "        0.04087844, 0.0523396 , 0.119414  , 0.1465951 , 0.03339591,\n",
       "        0.04542856, 0.04833632, 0.05776124, 0.13638592, 0.15014105,\n",
       "        0.04220629, 0.0460145 , 0.04262052, 0.05282574, 0.13769054,\n",
       "        0.143293  ]),\n",
       " 'mean_score_time': array([0.00640874, 0.00591593, 0.00631747, 0.0065321 , 0.00670223,\n",
       "        0.00611715, 0.00611787, 0.0061132 , 0.00620236, 0.00610504,\n",
       "        0.00622306, 0.00656538, 0.00619555, 0.00609775, 0.00614543,\n",
       "        0.00731936, 0.00844097, 0.00853367, 0.0065114 , 0.00644546,\n",
       "        0.0071312 , 0.00765157, 0.00854135, 0.00822153, 0.00691895,\n",
       "        0.00791807, 0.00752058, 0.00782833, 0.00917983, 0.00873108,\n",
       "        0.00755119, 0.00732651, 0.00732408, 0.00801759, 0.00832243,\n",
       "        0.00812674]),\n",
       " 'mean_test_score': array([0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.64583333, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.6953125 , 0.69791667, 0.54166667,\n",
       "        0.53385417]),\n",
       " 'mean_train_score': array([0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.5371777 , 0.73766313, 0.9525008 , 1.        ,\n",
       "        0.53393884, 0.54170675, 0.76237661, 0.97595312, 1.        ,\n",
       "        1.        ]),\n",
       " 'param_C': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1, 1,\n",
       "                    1, 1, 1, 1, 10, 10, 10, 10, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_gamma': masked_array(data=[0.0001, 0.001, 0.01, 0.1, 1, 10, 0.0001, 0.001, 0.01,\n",
       "                    0.1, 1, 10, 0.0001, 0.001, 0.01, 0.1, 1, 10, 0.0001,\n",
       "                    0.001, 0.01, 0.1, 1, 10, 0.0001, 0.001, 0.01, 0.1, 1,\n",
       "                    10, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.0001, 'gamma': 0.0001},\n",
       "  {'C': 0.0001, 'gamma': 0.001},\n",
       "  {'C': 0.0001, 'gamma': 0.01},\n",
       "  {'C': 0.0001, 'gamma': 0.1},\n",
       "  {'C': 0.0001, 'gamma': 1},\n",
       "  {'C': 0.0001, 'gamma': 10},\n",
       "  {'C': 0.001, 'gamma': 0.0001},\n",
       "  {'C': 0.001, 'gamma': 0.001},\n",
       "  {'C': 0.001, 'gamma': 0.01},\n",
       "  {'C': 0.001, 'gamma': 0.1},\n",
       "  {'C': 0.001, 'gamma': 1},\n",
       "  {'C': 0.001, 'gamma': 10},\n",
       "  {'C': 0.01, 'gamma': 0.0001},\n",
       "  {'C': 0.01, 'gamma': 0.001},\n",
       "  {'C': 0.01, 'gamma': 0.01},\n",
       "  {'C': 0.01, 'gamma': 0.1},\n",
       "  {'C': 0.01, 'gamma': 1},\n",
       "  {'C': 0.01, 'gamma': 10},\n",
       "  {'C': 0.1, 'gamma': 0.0001},\n",
       "  {'C': 0.1, 'gamma': 0.001},\n",
       "  {'C': 0.1, 'gamma': 0.01},\n",
       "  {'C': 0.1, 'gamma': 0.1},\n",
       "  {'C': 0.1, 'gamma': 1},\n",
       "  {'C': 0.1, 'gamma': 10},\n",
       "  {'C': 1, 'gamma': 0.0001},\n",
       "  {'C': 1, 'gamma': 0.001},\n",
       "  {'C': 1, 'gamma': 0.01},\n",
       "  {'C': 1, 'gamma': 0.1},\n",
       "  {'C': 1, 'gamma': 1},\n",
       "  {'C': 1, 'gamma': 10},\n",
       "  {'C': 10, 'gamma': 0.0001},\n",
       "  {'C': 10, 'gamma': 0.001},\n",
       "  {'C': 10, 'gamma': 0.01},\n",
       "  {'C': 10, 'gamma': 0.1},\n",
       "  {'C': 10, 'gamma': 1},\n",
       "  {'C': 10, 'gamma': 10}],\n",
       " 'rank_test_score': array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 2, 1, 4, 5]),\n",
       " 'split0_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.59756098, 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.68292683, 0.68292683, 0.51219512,\n",
       "        0.5       ]),\n",
       " 'split0_train_score': array([0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54635762, 0.73509934, 0.95033113, 1.        ,\n",
       "        0.54304636, 0.54635762, 0.76490066, 0.97682119, 1.        ,\n",
       "        1.        ]),\n",
       " 'split1_test_score': array([0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.5125, 0.5125, 0.5125, 0.6375, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.675 , 0.7   , 0.525 , 0.5125]),\n",
       " 'split1_train_score': array([0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.74342105, 0.95723684, 1.        ,\n",
       "        0.53947368, 0.54276316, 0.75986842, 0.98026316, 1.        ,\n",
       "        1.        ]),\n",
       " 'split2_test_score': array([0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.61038961, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.66233766, 0.7012987 , 0.53246753,\n",
       "        0.53246753]),\n",
       " 'split2_train_score': array([0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53745928, 0.74267101, 0.95765472, 1.        ,\n",
       "        0.53420195, 0.54397394, 0.76221498, 0.98045603, 1.        ,\n",
       "        1.        ]),\n",
       " 'split3_test_score': array([0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.65753425, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.71232877, 0.68493151, 0.57534247,\n",
       "        0.56164384]),\n",
       " 'split3_train_score': array([0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.53054662, 0.74276527, 0.95176849, 1.        ,\n",
       "        0.52733119, 0.53697749, 0.76848875, 0.97427653, 1.        ,\n",
       "        1.        ]),\n",
       " 'split4_test_score': array([0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.73611111, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.75      , 0.72222222, 0.56944444,\n",
       "        0.56944444]),\n",
       " 'split4_train_score': array([0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.53205128, 0.72435897, 0.94551282, 1.        ,\n",
       "        0.52564103, 0.53846154, 0.75641026, 0.96794872, 1.        ,\n",
       "        1.        ]),\n",
       " 'std_fit_time': array([0.00254981, 0.00168278, 0.00092961, 0.00176527, 0.00097134,\n",
       "        0.00067682, 0.00083902, 0.00116448, 0.00047881, 0.00088256,\n",
       "        0.00108832, 0.00166989, 0.00082906, 0.00102198, 0.00290371,\n",
       "        0.00113421, 0.0035975 , 0.00301895, 0.00109633, 0.00120742,\n",
       "        0.00219811, 0.00093031, 0.00396137, 0.00236241, 0.00231145,\n",
       "        0.00251793, 0.00346791, 0.0012507 , 0.00233793, 0.0034917 ,\n",
       "        0.00116061, 0.00189268, 0.00105575, 0.00215969, 0.00596389,\n",
       "        0.0045374 ]),\n",
       " 'std_score_time': array([0.00096534, 0.00037552, 0.00040101, 0.00055499, 0.00028108,\n",
       "        0.00020049, 0.00037652, 0.00037168, 0.00051926, 0.00021641,\n",
       "        0.00025643, 0.00043761, 0.00038737, 0.00017823, 0.0002247 ,\n",
       "        0.0002453 , 0.00035084, 0.00033401, 0.0005434 , 0.00040114,\n",
       "        0.00019649, 0.00051637, 0.0003179 , 0.00040243, 0.00037564,\n",
       "        0.0005932 , 0.00063569, 0.00038676, 0.00239981, 0.00051402,\n",
       "        0.00033528, 0.00060972, 0.00024031, 0.0005494 , 0.00040118,\n",
       "        0.00058795]),\n",
       " 'std_test_score': array([0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.04814706, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.03086351, 0.01392844, 0.02491146,\n",
       "        0.02691538]),\n",
       " 'std_train_score': array([0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00565461, 0.00731899, 0.00454023, 0.        ,\n",
       "        0.00672555, 0.00348666, 0.00413868, 0.00461492, 0.        ,\n",
       "        0.        ])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.1}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameters : C=0.0001, gamma=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6979166666666666"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "KSVC_clf = svm.SVC(kernel='rbf',C=10,gamma=0.1)\n",
    "\n",
    "KSVC_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.9765625\n",
      "Test Recall score: 0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = KSVC_clf.predict(X_train)\n",
    "y_pred_test = KSVC_clf.predict(X_test)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that test score is poor and Kernalised SVM doesn't perform well.\n",
    "- Also model is overfitting as there is large difference between train and test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [2, 3, 4, 5, 6, 10, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "param_grid = {'max_depth': [2, 3, 4, 5,6, 10, 20]}\n",
    "\n",
    "grid_search = GridSearchCV(dt_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00532427, 0.00743113, 0.00912385, 0.01062641, 0.01261783,\n",
       "        0.01836014, 0.02586493]),\n",
       " 'mean_score_time': array([0.00080953, 0.00059462, 0.00040293, 0.0004971 , 0.00051208,\n",
       "        0.00090146, 0.00070386]),\n",
       " 'mean_test_score': array([0.6171875 , 0.6484375 , 0.671875  , 0.65104167, 0.68489583,\n",
       "        0.65885417, 0.65104167]),\n",
       " 'mean_train_score': array([0.64791021, 0.71820893, 0.7721957 , 0.81845886, 0.8627067 ,\n",
       "        0.9669056 , 1.        ]),\n",
       " 'param_max_depth': masked_array(data=[2, 3, 4, 5, 6, 10, 20],\n",
       "              mask=[False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 2},\n",
       "  {'max_depth': 3},\n",
       "  {'max_depth': 4},\n",
       "  {'max_depth': 5},\n",
       "  {'max_depth': 6},\n",
       "  {'max_depth': 10},\n",
       "  {'max_depth': 20}],\n",
       " 'rank_test_score': array([7, 6, 2, 4, 1, 3, 4]),\n",
       " 'split0_test_score': array([0.56097561, 0.6097561 , 0.63414634, 0.6097561 , 0.67073171,\n",
       "        0.69512195, 0.63414634]),\n",
       " 'split0_train_score': array([0.66225166, 0.71192053, 0.7615894 , 0.81125828, 0.86092715,\n",
       "        0.97682119, 1.        ]),\n",
       " 'split1_test_score': array([0.575 , 0.6   , 0.65  , 0.6375, 0.7375, 0.625 , 0.6875]),\n",
       " 'split1_train_score': array([0.64802632, 0.74013158, 0.78618421, 0.82894737, 0.86513158,\n",
       "        0.97368421, 1.        ]),\n",
       " 'split2_test_score': array([0.5974026 , 0.64935065, 0.67532468, 0.64935065, 0.62337662,\n",
       "        0.62337662, 0.61038961]),\n",
       " 'split2_train_score': array([0.66123779, 0.73615635, 0.79478827, 0.8534202 , 0.89250814,\n",
       "        0.97394137, 1.        ]),\n",
       " 'split3_test_score': array([0.68493151, 0.69863014, 0.68493151, 0.65753425, 0.64383562,\n",
       "        0.63013699, 0.64383562]),\n",
       " 'split3_train_score': array([0.62700965, 0.68488746, 0.74598071, 0.78456592, 0.82958199,\n",
       "        0.94533762, 1.        ]),\n",
       " 'split4_test_score': array([0.68055556, 0.69444444, 0.72222222, 0.70833333, 0.75      ,\n",
       "        0.72222222, 0.68055556]),\n",
       " 'split4_train_score': array([0.64102564, 0.71794872, 0.7724359 , 0.81410256, 0.86538462,\n",
       "        0.96474359, 1.        ]),\n",
       " 'std_fit_time': array([0.00059198, 0.0004894 , 0.00020248, 0.00049376, 0.00084751,\n",
       "        0.00049501, 0.0022111 ]),\n",
       " 'std_score_time': array([2.52358947e-04, 4.85673194e-04, 3.76033765e-04, 1.66818057e-05,\n",
       "        1.43141718e-05, 2.03225905e-04, 2.46706228e-04]),\n",
       " 'std_test_score': array([0.05243442, 0.04103984, 0.03022803, 0.03203451, 0.04993464,\n",
       "        0.04100882, 0.029025  ]),\n",
       " 'std_train_score': array([0.01317229, 0.0197618 , 0.01735442, 0.02259142, 0.02000795,\n",
       "        0.01152061, 0.        ])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameter : max_depth = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6848958333333334"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(max_depth=4)\n",
    "dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.75\n",
      "Test Recall score: 0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = dt_clf.predict(X_train)\n",
    "y_pred_test = dt_clf.predict(X_test)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that test score is poor and decision tree doesn't perform well.\n",
    "- Also model is somewhat overfitting as there is difference between train and test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
       "            oob_score=False, random_state=10, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_features': [100, 125, 150, 200], 'max_depth': [6, 8, 10, 12, 14], 'max_leaf_nodes': [20, 22, 30, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=300, criterion='gini',n_jobs= -1,random_state=10)\n",
    "\n",
    "param_grid = {'max_features': [100,125,150,200],\n",
    "          'max_depth': [6,8,10,12,14],\n",
    "           'max_leaf_nodes':[20,22,30,50]}\n",
    "\n",
    "grid_search = GridSearchCV(rf_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.66322918, 0.61492476, 0.62522388, 0.58289509, 0.73230014,\n",
       "        0.71308217, 0.63803368, 0.58591447, 0.78277106, 0.77338352,\n",
       "        0.75089378, 1.11470785, 1.45919003, 1.37811384, 1.38536725,\n",
       "        1.39709105, 1.0284586 , 1.05067487, 1.0286581 , 1.01318107,\n",
       "        1.13163071, 1.2091054 , 1.17615852, 1.13750777, 1.24722939,\n",
       "        1.27541456, 1.33031244, 1.32072577, 1.5391016 , 1.55326967,\n",
       "        1.5533196 , 1.55566206, 1.07053404, 1.03641376, 1.13707972,\n",
       "        1.13692846, 1.19426069, 1.18031106, 1.25349665, 1.31758909,\n",
       "        1.33598619, 1.35012279, 1.39897032, 1.44797058, 1.58391948,\n",
       "        1.5546061 , 1.63191838, 1.66191134, 1.04788566, 1.14922872,\n",
       "        1.07139239, 1.12836003, 1.12947664, 1.18083415, 1.2199707 ,\n",
       "        1.30580335, 1.2920979 , 1.33420339, 1.38217564, 1.44151683,\n",
       "        1.5253149 , 1.52792044, 1.57850747, 1.6675765 , 1.07427979,\n",
       "        1.09619226, 1.14007459, 1.17078614, 1.19782419, 1.22443323,\n",
       "        1.26182323, 1.33282828, 1.32292137, 1.33803821, 1.41246548,\n",
       "        1.43924875, 1.52499881, 1.59649439, 1.71400471, 1.79391861]),\n",
       " 'mean_score_time': array([0.10412655, 0.10514112, 0.10475674, 0.10444312, 0.10450382,\n",
       "        0.10412717, 0.10440316, 0.10539727, 0.10605788, 0.10658288,\n",
       "        0.10496626, 0.17239861, 0.17288218, 0.17169328, 0.17220149,\n",
       "        0.21315203, 0.23414297, 0.19304633, 0.23561959, 0.19077048,\n",
       "        0.21473918, 0.1317162 , 0.1510963 , 0.19164543, 0.15050359,\n",
       "        0.27740412, 0.25659065, 0.17252131, 0.15232182, 0.1724792 ,\n",
       "        0.19181848, 0.10760603, 0.19102139, 0.14914975, 0.17228856,\n",
       "        0.19508758, 0.10704799, 0.10940599, 0.19249711, 0.19299788,\n",
       "        0.17106743, 0.15273056, 0.16993308, 0.11077156, 0.17304602,\n",
       "        0.15035849, 0.21317105, 0.19378567, 0.17038312, 0.23374543,\n",
       "        0.21411376, 0.27881722, 0.25348177, 0.21243439, 0.15211115,\n",
       "        0.15249529, 0.21453438, 0.19420743, 0.2366395 , 0.29808664,\n",
       "        0.19184337, 0.15134892, 0.15049357, 0.19353004, 0.19291954,\n",
       "        0.25525684, 0.17099957, 0.17176051, 0.19357171, 0.15107684,\n",
       "        0.17183299, 0.1716651 , 0.23656907, 0.27957377, 0.21457567,\n",
       "        0.10913901, 0.10898228, 0.19513245, 0.19291787, 0.15145345]),\n",
       " 'mean_test_score': array([0.73177083, 0.734375  , 0.73177083, 0.73177083, 0.7265625 ,\n",
       "        0.7265625 , 0.72916667, 0.72916667, 0.74479167, 0.74479167,\n",
       "        0.74479167, 0.74479167, 0.734375  , 0.7421875 , 0.734375  ,\n",
       "        0.734375  , 0.75      , 0.75260417, 0.75      , 0.75      ,\n",
       "        0.7421875 , 0.7421875 , 0.74739583, 0.75      , 0.75      ,\n",
       "        0.75      , 0.75260417, 0.75260417, 0.74739583, 0.75      ,\n",
       "        0.75260417, 0.75      , 0.75      , 0.75260417, 0.7578125 ,\n",
       "        0.75520833, 0.7421875 , 0.74479167, 0.74479167, 0.74739583,\n",
       "        0.75      , 0.75      , 0.75260417, 0.75260417, 0.74739583,\n",
       "        0.75      , 0.75      , 0.75260417, 0.75      , 0.75260417,\n",
       "        0.75520833, 0.75520833, 0.7421875 , 0.74739583, 0.74739583,\n",
       "        0.74479167, 0.75      , 0.75      , 0.75      , 0.75260417,\n",
       "        0.74739583, 0.75      , 0.75      , 0.75      , 0.75      ,\n",
       "        0.75260417, 0.75520833, 0.75260417, 0.7421875 , 0.74739583,\n",
       "        0.74739583, 0.74479167, 0.75      , 0.75      , 0.75      ,\n",
       "        0.75260417, 0.74739583, 0.75      , 0.75      , 0.74739583]),\n",
       " 'mean_train_score': array([0.89262431, 0.90885622, 0.9225074 , 0.92507562, 0.89910615,\n",
       "        0.91475282, 0.93029482, 0.93288221, 0.90175343, 0.91604118,\n",
       "        0.92900863, 0.93029481, 0.90566235, 0.91869982, 0.93163792,\n",
       "        0.93358634, 0.89907884, 0.91142905, 0.95962135, 0.97330675,\n",
       "        0.90102085, 0.915381  , 0.96091213, 0.97133514, 0.90630958,\n",
       "        0.91934523, 0.96287536, 0.97323675, 0.9076125 , 0.92515196,\n",
       "        0.96811108, 0.97783786, 0.89912968, 0.91604738, 0.97271416,\n",
       "        0.99153174, 0.90367159, 0.91737154, 0.98114672, 0.98958778,\n",
       "        0.90629271, 0.92258166, 0.97983943, 0.99153174, 0.9075977 ,\n",
       "        0.92844578, 0.98443446, 0.99218963, 0.89980881, 0.91735032,\n",
       "        0.97596551, 0.99350978, 0.90365679, 0.91801865, 0.98312706,\n",
       "        0.99350978, 0.90695061, 0.92389745, 0.98506264, 0.9941508 ,\n",
       "        0.9082556 , 0.92975915, 0.98766896, 0.9941508 , 0.89980881,\n",
       "        0.91735032, 0.97985389, 0.99547095, 0.90365679, 0.91801865,\n",
       "        0.98442161, 0.99547095, 0.90695061, 0.92323956, 0.98442161,\n",
       "        0.99547095, 0.9082556 , 0.92975915, 0.98831204, 0.99675506]),\n",
       " 'param_max_depth': masked_array(data=[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8,\n",
       "                    8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "                    14, 14, 14],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=[100, 100, 100, 100, 125, 125, 125, 125, 150, 150, 150,\n",
       "                    150, 200, 200, 200, 200, 100, 100, 100, 100, 125, 125,\n",
       "                    125, 125, 150, 150, 150, 150, 200, 200, 200, 200, 100,\n",
       "                    100, 100, 100, 125, 125, 125, 125, 150, 150, 150, 150,\n",
       "                    200, 200, 200, 200, 100, 100, 100, 100, 125, 125, 125,\n",
       "                    125, 150, 150, 150, 150, 200, 200, 200, 200, 100, 100,\n",
       "                    100, 100, 125, 125, 125, 125, 150, 150, 150, 150, 200,\n",
       "                    200, 200, 200],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_leaf_nodes': masked_array(data=[20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22,\n",
       "                    30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50,\n",
       "                    20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22,\n",
       "                    30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50,\n",
       "                    20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22,\n",
       "                    30, 50, 20, 22, 30, 50, 20, 22, 30, 50],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 6, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 6, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 6, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 6, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 6, 'max_features': 125, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 6, 'max_features': 125, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 6, 'max_features': 125, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 6, 'max_features': 125, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 6, 'max_features': 150, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 6, 'max_features': 150, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 6, 'max_features': 150, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 6, 'max_features': 150, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 6, 'max_features': 200, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 6, 'max_features': 200, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 6, 'max_features': 200, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 6, 'max_features': 200, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 8, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 8, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 8, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 8, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 8, 'max_features': 125, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 8, 'max_features': 125, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 8, 'max_features': 125, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 8, 'max_features': 125, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 8, 'max_features': 150, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 8, 'max_features': 150, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 8, 'max_features': 150, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 8, 'max_features': 150, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 8, 'max_features': 200, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 8, 'max_features': 200, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 8, 'max_features': 200, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 8, 'max_features': 200, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 10, 'max_features': 125, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 10, 'max_features': 125, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 10, 'max_features': 125, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 10, 'max_features': 125, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 10, 'max_features': 150, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 10, 'max_features': 150, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 10, 'max_features': 150, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 10, 'max_features': 150, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 10, 'max_features': 200, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 10, 'max_features': 200, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 10, 'max_features': 200, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 10, 'max_features': 200, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 12, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 12, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 12, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 12, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 12, 'max_features': 125, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 12, 'max_features': 125, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 12, 'max_features': 125, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 12, 'max_features': 125, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 12, 'max_features': 150, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 12, 'max_features': 150, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 12, 'max_features': 150, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 12, 'max_features': 150, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 12, 'max_features': 200, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 12, 'max_features': 200, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 12, 'max_features': 200, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 12, 'max_features': 200, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 14, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 14, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 14, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 14, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 14, 'max_features': 125, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 14, 'max_features': 125, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 14, 'max_features': 125, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 14, 'max_features': 125, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 14, 'max_features': 150, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 14, 'max_features': 150, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 14, 'max_features': 150, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 14, 'max_features': 150, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 14, 'max_features': 200, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 14, 'max_features': 200, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 14, 'max_features': 200, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 14, 'max_features': 200, 'max_leaf_nodes': 50}],\n",
       " 'rank_test_score': array([74, 70, 74, 74, 79, 79, 77, 77, 56, 56, 56, 56, 70, 64, 70, 70, 19,\n",
       "         6, 19, 19, 64, 64, 45, 19, 19, 19,  6,  6, 45, 19,  6, 19, 19,  6,\n",
       "         1,  2, 64, 56, 56, 45, 19, 19,  6,  6, 45, 19, 19,  6, 19,  6,  2,\n",
       "         2, 64, 45, 45, 56, 19, 19, 19,  6, 45, 19, 19, 19, 19,  6,  2,  6,\n",
       "        64, 45, 45, 56, 19, 19, 19,  6, 45, 19, 19, 45]),\n",
       " 'split0_test_score': array([0.68292683, 0.68292683, 0.68292683, 0.68292683, 0.67073171,\n",
       "        0.67073171, 0.67073171, 0.67073171, 0.7195122 , 0.7195122 ,\n",
       "        0.7195122 , 0.7195122 , 0.69512195, 0.70731707, 0.69512195,\n",
       "        0.69512195, 0.7195122 , 0.7195122 , 0.70731707, 0.70731707,\n",
       "        0.69512195, 0.69512195, 0.70731707, 0.70731707, 0.73170732,\n",
       "        0.73170732, 0.73170732, 0.73170732, 0.7195122 , 0.73170732,\n",
       "        0.73170732, 0.73170732, 0.7195122 , 0.7195122 , 0.73170732,\n",
       "        0.73170732, 0.69512195, 0.70731707, 0.70731707, 0.7195122 ,\n",
       "        0.73170732, 0.73170732, 0.73170732, 0.73170732, 0.7195122 ,\n",
       "        0.73170732, 0.73170732, 0.73170732, 0.7195122 , 0.7195122 ,\n",
       "        0.73170732, 0.73170732, 0.69512195, 0.70731707, 0.7195122 ,\n",
       "        0.7195122 , 0.73170732, 0.73170732, 0.73170732, 0.73170732,\n",
       "        0.7195122 , 0.73170732, 0.73170732, 0.73170732, 0.7195122 ,\n",
       "        0.7195122 , 0.73170732, 0.73170732, 0.69512195, 0.70731707,\n",
       "        0.7195122 , 0.7195122 , 0.73170732, 0.73170732, 0.73170732,\n",
       "        0.73170732, 0.7195122 , 0.73170732, 0.73170732, 0.73170732]),\n",
       " 'split0_train_score': array([0.89735099, 0.90728477, 0.9205298 , 0.9205298 , 0.90066225,\n",
       "        0.92384106, 0.93046358, 0.93377483, 0.9205298 , 0.92715232,\n",
       "        0.93377483, 0.93377483, 0.91721854, 0.92715232, 0.93046358,\n",
       "        0.93377483, 0.90066225, 0.91059603, 0.95695364, 0.97350993,\n",
       "        0.90066225, 0.9205298 , 0.9602649 , 0.97019868, 0.91059603,\n",
       "        0.92384106, 0.96357616, 0.96357616, 0.91390728, 0.92384106,\n",
       "        0.97019868, 0.97350993, 0.90397351, 0.9205298 , 0.97350993,\n",
       "        0.99337748, 0.90397351, 0.92384106, 0.98344371, 0.99337748,\n",
       "        0.91059603, 0.92715232, 0.98013245, 0.99337748, 0.91059603,\n",
       "        0.93046358, 0.99337748, 0.99337748, 0.90728477, 0.9205298 ,\n",
       "        0.97682119, 0.99668874, 0.90397351, 0.9205298 , 0.99006623,\n",
       "        0.99668874, 0.91059603, 0.92715232, 0.99006623, 0.99668874,\n",
       "        0.91059603, 0.93046358, 0.99337748, 0.99668874, 0.90728477,\n",
       "        0.9205298 , 0.98013245, 1.        , 0.90397351, 0.9205298 ,\n",
       "        0.99006623, 1.        , 0.91059603, 0.92715232, 0.99006623,\n",
       "        1.        , 0.91059603, 0.93046358, 0.99337748, 1.        ]),\n",
       " 'split1_test_score': array([0.7   , 0.7   , 0.7   , 0.7   , 0.7   , 0.7   , 0.7   , 0.7   ,\n",
       "        0.7125, 0.7125, 0.7125, 0.7125, 0.7   , 0.7125, 0.6875, 0.6875,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.7125, 0.7125, 0.725 , 0.725 ,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 ,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.7125, 0.7125, 0.725 , 0.725 ,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 ,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.7125, 0.725 , 0.725 , 0.725 ,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.7125, 0.7125,\n",
       "        0.725 , 0.725 , 0.725 , 0.7125, 0.7125, 0.725 , 0.725 , 0.725 ,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.7125, 0.7125]),\n",
       " 'split1_train_score': array([0.88815789, 0.90460526, 0.91447368, 0.91447368, 0.89473684,\n",
       "        0.91118421, 0.91776316, 0.91776316, 0.88815789, 0.90460526,\n",
       "        0.91118421, 0.91118421, 0.89802632, 0.91776316, 0.92763158,\n",
       "        0.92763158, 0.89144737, 0.90460526, 0.95394737, 0.97039474,\n",
       "        0.89473684, 0.91118421, 0.95394737, 0.96710526, 0.91118421,\n",
       "        0.92763158, 0.95723684, 0.97039474, 0.90460526, 0.92763158,\n",
       "        0.96381579, 0.97368421, 0.89802632, 0.91776316, 0.97697368,\n",
       "        0.98684211, 0.90789474, 0.91776316, 0.98026316, 0.98684211,\n",
       "        0.90789474, 0.92105263, 0.98355263, 0.98684211, 0.90789474,\n",
       "        0.93421053, 0.98355263, 0.99013158, 0.90131579, 0.91776316,\n",
       "        0.97697368, 0.99342105, 0.90460526, 0.92105263, 0.98355263,\n",
       "        0.99342105, 0.91118421, 0.92763158, 0.98355263, 0.99342105,\n",
       "        0.91118421, 0.93421053, 0.98684211, 0.99342105, 0.90131579,\n",
       "        0.91776316, 0.98026316, 0.99671053, 0.90460526, 0.92105263,\n",
       "        0.98355263, 0.99671053, 0.91118421, 0.92434211, 0.98355263,\n",
       "        0.99671053, 0.91118421, 0.93421053, 0.98684211, 0.99671053]),\n",
       " 'split2_test_score': array([0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987]),\n",
       " 'split2_train_score': array([0.90553746, 0.91856678, 0.93159609, 0.93159609, 0.90879479,\n",
       "        0.91530945, 0.93485342, 0.93485342, 0.90553746, 0.9218241 ,\n",
       "        0.93811075, 0.93811075, 0.91530945, 0.92833876, 0.93811075,\n",
       "        0.93811075, 0.90553746, 0.91530945, 0.96742671, 0.9771987 ,\n",
       "        0.90553746, 0.91856678, 0.96416938, 0.97394137, 0.90879479,\n",
       "        0.9218241 , 0.96416938, 0.97394137, 0.91856678, 0.93159609,\n",
       "        0.97394137, 0.98371336, 0.90553746, 0.91530945, 0.98371336,\n",
       "        0.99348534, 0.90553746, 0.9218241 , 0.98697068, 0.98697068,\n",
       "        0.90879479, 0.93485342, 0.98045603, 0.99348534, 0.91530945,\n",
       "        0.93485342, 0.98697068, 0.99348534, 0.90553746, 0.9218241 ,\n",
       "        0.99022801, 0.99348534, 0.90553746, 0.92508143, 0.98697068,\n",
       "        0.99348534, 0.90879479, 0.93485342, 0.99022801, 0.99348534,\n",
       "        0.91530945, 0.94462541, 0.99022801, 0.99348534, 0.90553746,\n",
       "        0.9218241 , 0.99022801, 0.99348534, 0.90553746, 0.92508143,\n",
       "        0.99022801, 0.99348534, 0.90879479, 0.93485342, 0.99022801,\n",
       "        0.99348534, 0.91530945, 0.94462541, 0.99022801, 0.99348534]),\n",
       " 'split3_test_score': array([0.78082192, 0.78082192, 0.78082192, 0.78082192, 0.79452055,\n",
       "        0.79452055, 0.79452055, 0.79452055, 0.80821918, 0.80821918,\n",
       "        0.80821918, 0.80821918, 0.79452055, 0.79452055, 0.79452055,\n",
       "        0.79452055, 0.80821918, 0.80821918, 0.80821918, 0.80821918,\n",
       "        0.80821918, 0.80821918, 0.80821918, 0.80821918, 0.79452055,\n",
       "        0.79452055, 0.79452055, 0.79452055, 0.79452055, 0.79452055,\n",
       "        0.79452055, 0.79452055, 0.80821918, 0.80821918, 0.80821918,\n",
       "        0.80821918, 0.80821918, 0.80821918, 0.79452055, 0.78082192,\n",
       "        0.79452055, 0.79452055, 0.79452055, 0.79452055, 0.79452055,\n",
       "        0.79452055, 0.79452055, 0.79452055, 0.80821918, 0.80821918,\n",
       "        0.80821918, 0.79452055, 0.80821918, 0.80821918, 0.79452055,\n",
       "        0.78082192, 0.79452055, 0.79452055, 0.79452055, 0.79452055,\n",
       "        0.79452055, 0.79452055, 0.79452055, 0.79452055, 0.80821918,\n",
       "        0.80821918, 0.80821918, 0.79452055, 0.80821918, 0.80821918,\n",
       "        0.79452055, 0.78082192, 0.79452055, 0.79452055, 0.79452055,\n",
       "        0.79452055, 0.79452055, 0.79452055, 0.79452055, 0.79452055]),\n",
       " 'split3_train_score': array([0.88745981, 0.91318328, 0.93247588, 0.93890675, 0.89710611,\n",
       "        0.90996785, 0.93890675, 0.94212219, 0.89710611, 0.90675241,\n",
       "        0.93569132, 0.94212219, 0.90032154, 0.91639871, 0.93890675,\n",
       "        0.94533762, 0.89710611, 0.90996785, 0.97106109, 0.97427653,\n",
       "        0.90032154, 0.90675241, 0.96784566, 0.97427653, 0.90353698,\n",
       "        0.90996785, 0.97106109, 0.9807074 , 0.90675241, 0.91961415,\n",
       "        0.97106109, 0.98392283, 0.89067524, 0.90996785, 0.96463023,\n",
       "        0.99356913, 0.89710611, 0.90996785, 0.97749196, 0.99356913,\n",
       "        0.90353698, 0.91318328, 0.97749196, 0.99356913, 0.90675241,\n",
       "        0.92282958, 0.9807074 , 0.99356913, 0.89067524, 0.90996785,\n",
       "        0.97106109, 0.99356913, 0.90032154, 0.90996785, 0.97427653,\n",
       "        0.99356913, 0.90353698, 0.91318328, 0.97749196, 0.99356913,\n",
       "        0.90675241, 0.92282958, 0.98392283, 0.99356913, 0.89067524,\n",
       "        0.90996785, 0.97749196, 0.99356913, 0.90032154, 0.90996785,\n",
       "        0.97749196, 0.99356913, 0.90353698, 0.91318328, 0.97749196,\n",
       "        0.99356913, 0.90675241, 0.92282958, 0.98713826, 0.99678457]),\n",
       " 'split4_test_score': array([0.80555556, 0.81944444, 0.80555556, 0.80555556, 0.77777778,\n",
       "        0.77777778, 0.79166667, 0.79166667, 0.79166667, 0.79166667,\n",
       "        0.79166667, 0.79166667, 0.79166667, 0.80555556, 0.80555556,\n",
       "        0.80555556, 0.80555556, 0.81944444, 0.81944444, 0.81944444,\n",
       "        0.80555556, 0.80555556, 0.80555556, 0.81944444, 0.80555556,\n",
       "        0.80555556, 0.81944444, 0.81944444, 0.80555556, 0.80555556,\n",
       "        0.81944444, 0.80555556, 0.80555556, 0.81944444, 0.83333333,\n",
       "        0.81944444, 0.80555556, 0.80555556, 0.80555556, 0.81944444,\n",
       "        0.80555556, 0.80555556, 0.81944444, 0.81944444, 0.80555556,\n",
       "        0.80555556, 0.80555556, 0.81944444, 0.80555556, 0.81944444,\n",
       "        0.81944444, 0.83333333, 0.80555556, 0.80555556, 0.80555556,\n",
       "        0.80555556, 0.80555556, 0.80555556, 0.80555556, 0.81944444,\n",
       "        0.80555556, 0.80555556, 0.81944444, 0.81944444, 0.80555556,\n",
       "        0.81944444, 0.81944444, 0.83333333, 0.80555556, 0.80555556,\n",
       "        0.80555556, 0.80555556, 0.80555556, 0.80555556, 0.80555556,\n",
       "        0.81944444, 0.80555556, 0.80555556, 0.81944444, 0.80555556]),\n",
       " 'split4_train_score': array([0.88461538, 0.90064103, 0.91346154, 0.91987179, 0.89423077,\n",
       "        0.91346154, 0.92948718, 0.93589744, 0.8974359 , 0.91987179,\n",
       "        0.92628205, 0.92628205, 0.8974359 , 0.90384615, 0.92307692,\n",
       "        0.92307692, 0.90064103, 0.91666667, 0.94871795, 0.97115385,\n",
       "        0.90384615, 0.91987179, 0.95833333, 0.97115385, 0.8974359 ,\n",
       "        0.91346154, 0.95833333, 0.9775641 , 0.89423077, 0.92307692,\n",
       "        0.96153846, 0.97435897, 0.8974359 , 0.91666667, 0.96474359,\n",
       "        0.99038462, 0.90384615, 0.91346154, 0.9775641 , 0.98717949,\n",
       "        0.90064103, 0.91666667, 0.9775641 , 0.99038462, 0.8974359 ,\n",
       "        0.91987179, 0.9775641 , 0.99038462, 0.89423077, 0.91666667,\n",
       "        0.96474359, 0.99038462, 0.90384615, 0.91346154, 0.98076923,\n",
       "        0.99038462, 0.90064103, 0.91666667, 0.98397436, 0.99358974,\n",
       "        0.8974359 , 0.91666667, 0.98397436, 0.99358974, 0.89423077,\n",
       "        0.91666667, 0.97115385, 0.99358974, 0.90384615, 0.91346154,\n",
       "        0.98076923, 0.99358974, 0.90064103, 0.91666667, 0.98076923,\n",
       "        0.99358974, 0.8974359 , 0.91666667, 0.98397436, 0.99679487]),\n",
       " 'std_fit_time': array([0.06366379, 0.0170098 , 0.06001357, 0.01873177, 0.0112431 ,\n",
       "        0.04416781, 0.11838059, 0.04669996, 0.12269318, 0.0667217 ,\n",
       "        0.09597018, 0.22040674, 0.02553451, 0.08480244, 0.04594559,\n",
       "        0.05610904, 0.07026247, 0.07198976, 0.03213894, 0.01950375,\n",
       "        0.00771984, 0.06055249, 0.03485912, 0.02813691, 0.07110509,\n",
       "        0.04900308, 0.06268012, 0.02729819, 0.0484227 , 0.03859704,\n",
       "        0.01172721, 0.04075435, 0.04261961, 0.02071827, 0.02586741,\n",
       "        0.01619115, 0.03888867, 0.04565239, 0.02897508, 0.06280753,\n",
       "        0.01713001, 0.01852109, 0.04557471, 0.01900548, 0.03213287,\n",
       "        0.02066187, 0.09437734, 0.09705805, 0.00587318, 0.15070856,\n",
       "        0.05072781, 0.07334791, 0.02724371, 0.05640251, 0.07707343,\n",
       "        0.04039832, 0.04812716, 0.05866733, 0.0758957 , 0.07860068,\n",
       "        0.07385025, 0.08279298, 0.03356513, 0.15915784, 0.04756225,\n",
       "        0.05766332, 0.02873111, 0.07341911, 0.04032098, 0.01089588,\n",
       "        0.07244833, 0.03057117, 0.06121558, 0.07763659, 0.07151487,\n",
       "        0.09142864, 0.06692526, 0.04886   , 0.06180322, 0.05585477]),\n",
       " 'std_score_time': array([0.00034934, 0.0009976 , 0.00179584, 0.0011279 , 0.00017053,\n",
       "        0.0003384 , 0.00047391, 0.00137971, 0.00165027, 0.00088696,\n",
       "        0.00149255, 0.05397063, 0.05171934, 0.0851663 , 0.0843999 ,\n",
       "        0.06670433, 0.07955148, 0.07982717, 0.10374799, 0.10291259,\n",
       "        0.06773241, 0.04149694, 0.0813716 , 0.04165025, 0.08586489,\n",
       "        0.05213134, 0.08531692, 0.05146328, 0.05148941, 0.08426006,\n",
       "        0.04212007, 0.00249027, 0.04155341, 0.05313283, 0.04927709,\n",
       "        0.07892298, 0.00173926, 0.00339898, 0.07786161, 0.07859325,\n",
       "        0.08331684, 0.05041775, 0.05172293, 0.00133856, 0.05059367,\n",
       "        0.08141402, 0.09249572, 0.07814236, 0.08119993, 0.10374897,\n",
       "        0.06641095, 0.08571447, 0.08381188, 0.09297793, 0.08378353,\n",
       "        0.08511559, 0.0917757 , 0.07814475, 0.07781746, 0.04417903,\n",
       "        0.08135847, 0.05152631, 0.0524894 , 0.07842872, 0.07863861,\n",
       "        0.08473709, 0.05071334, 0.08077215, 0.08005735, 0.08536133,\n",
       "        0.08498459, 0.05041497, 0.0780728 , 0.08575248, 0.06506415,\n",
       "        0.0016139 , 0.00264528, 0.08182548, 0.07902772, 0.05145927]),\n",
       " 'std_test_score': array([0.04882739, 0.0528942 , 0.04882739, 0.04882739, 0.04806635,\n",
       "        0.04806635, 0.05105397, 0.05105397, 0.04370129, 0.04370129,\n",
       "        0.04370129, 0.04370129, 0.04580083, 0.04529983, 0.05141855,\n",
       "        0.05141855, 0.04502612, 0.04843646, 0.05043235, 0.05043235,\n",
       "        0.05073156, 0.05073156, 0.04702254, 0.05043235, 0.04040752,\n",
       "        0.04040752, 0.04417587, 0.04417587, 0.04186907, 0.04040752,\n",
       "        0.04417587, 0.04040752, 0.04502612, 0.04843646, 0.05062276,\n",
       "        0.04689044, 0.05073156, 0.04851324, 0.04385456, 0.04338589,\n",
       "        0.04040752, 0.04040752, 0.04417587, 0.04417587, 0.04186907,\n",
       "        0.04040752, 0.04040752, 0.04417587, 0.04502612, 0.04843646,\n",
       "        0.04689044, 0.04825993, 0.05073156, 0.04702254, 0.04186907,\n",
       "        0.03919782, 0.04040752, 0.04040752, 0.04040752, 0.04417587,\n",
       "        0.04186907, 0.04040752, 0.04605485, 0.04605485, 0.04502612,\n",
       "        0.04843646, 0.04689044, 0.05012112, 0.05073156, 0.04702254,\n",
       "        0.04186907, 0.03919782, 0.04040752, 0.04040752, 0.04040752,\n",
       "        0.04417587, 0.04186907, 0.04040752, 0.04605485, 0.04229358]),\n",
       " 'std_train_score': array([0.00774409, 0.0063411 , 0.00815174, 0.00887651, 0.00535073,\n",
       "        0.00490315, 0.00710872, 0.00809613, 0.0108807 , 0.00881629,\n",
       "        0.00975057, 0.01090324, 0.00873063, 0.00884486, 0.00609022,\n",
       "        0.00780391, 0.0046646 , 0.0042868 , 0.00836644, 0.00241842,\n",
       "        0.00370104, 0.00545696, 0.00478039, 0.00263289, 0.00519117,\n",
       "        0.00659658, 0.00493142, 0.00594032, 0.00834835, 0.00410772,\n",
       "        0.00466265, 0.00489152, 0.00529313, 0.0034907 , 0.00732993,\n",
       "        0.00263374, 0.00359287, 0.00513685, 0.0036378 , 0.00317493,\n",
       "        0.00365735, 0.00770898, 0.00223346, 0.00263374, 0.00587411,\n",
       "        0.00605667, 0.00544574, 0.00158029, 0.00641116, 0.00412947,\n",
       "        0.00842796, 0.00199421, 0.00177181, 0.00549503, 0.00542214,\n",
       "        0.00199421, 0.00414915, 0.00789492, 0.00474334, 0.0012704 ,\n",
       "        0.00605205, 0.00959946, 0.00367432, 0.0012704 , 0.00641116,\n",
       "        0.00412947, 0.0061504 , 0.00257478, 0.00177181, 0.00549503,\n",
       "        0.00505353, 0.00257478, 0.00414915, 0.00769076, 0.00505353,\n",
       "        0.00257478, 0.00605205, 0.00959946, 0.00321508, 0.00206035])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 30}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameters : max_features=100, max_depth=10, max_leaf_nodes=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7578125"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features=100, max_leaf_nodes=30,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=300, criterion='gini',max_features=100,max_depth=10,max_leaf_nodes=30)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.9375\n",
      "Test Recall score: 0.75\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = rf_clf.predict(X_train)\n",
    "y_pred_test = rf_clf.predict(X_test)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that test score is good and Random forest performs well as it is ensemble method.\n",
    "- But model is overfitting as there is large difference between train and test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.91\n",
      "Test score: 0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "hard_voting_clf = VotingClassifier(estimators=[('knn', knn_clf),('lr',lreg_clf),('lsvc', LSVC_clf),\n",
    "                                   ('ksvc', KSVC_clf),('dt', dt_clf), ('rt', rf_clf)],voting = 'hard')\n",
    "hard_voting_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(hard_voting_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(hard_voting_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(estimator=hard_voting_clf,X=X_train,y=y_train, scoring='recall_weighted', cv=kFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.70\n",
      "Mean Std: 0.02\n"
     ]
    }
   ],
   "source": [
    "print('Mean Score: {0:0.2f}'.format(score.mean()))\n",
    "print('Mean Std: {0:0.2f}'.format(score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Machine Learning Models with Bagging and/or Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "bag_knn = BaggingClassifier(base_estimator=knn_clf, n_estimators=100,bootstrap_features=True, max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_knn, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.5448364251823543\n"
     ]
    }
   ],
   "source": [
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, Bagging is giving us a very low score. It doesn't improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasting with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "bag_knn = BaggingClassifier(base_estimator=knn_clf, n_estimators=100,bootstrap_features=True, bootstrap=False,\n",
    "                            max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_knn, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.5448364251823543\n"
     ]
    }
   ],
   "source": [
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasting is also giving us a very low score. It doesn't improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging with Logistic Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.5538831923686564\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_log = BaggingClassifier(base_estimator=lreg_clf, n_estimators=100,bootstrap_features=True, max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_log, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)\n",
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging algorithm decreased the variance, but raised the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging with Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.7018655577587445\n"
     ]
    }
   ],
   "source": [
    "bag_lsvc = BaggingClassifier(base_estimator=LSVC_clf, n_estimators=100,bootstrap_features=True, max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_lsvc, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)\n",
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging algorithm has raised the bias and doesn't improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging with SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.5578545673583478\n"
     ]
    }
   ],
   "source": [
    "bag_ksvc = BaggingClassifier(base_estimator=KSVC_clf, n_estimators=100,bootstrap_features=True, max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_ksvc, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)\n",
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging algorithm has raised the bias and doesn't improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.6245540394853004\n"
     ]
    }
   ],
   "source": [
    "bag_dt = BaggingClassifier(base_estimator=dt_clf, n_estimators=100,bootstrap_features=True, max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_dt, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)\n",
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging algorithm has raised the bias and doesn't improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.5848402123990685\n"
     ]
    }
   ],
   "source": [
    "bag_rf = BaggingClassifier(base_estimator=rf_clf, n_estimators=100,bootstrap_features=True, max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_rf, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)\n",
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging algorithm has raised the bias and doesn't improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive boosting with decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.99\n",
      "Test score: 0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = dt_clf, learning_rate = 0.5)\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(adaboost_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(adaboost_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adaptive boosting did in fact raise the average training accuracy \n",
    "for the Decision Tree but the test accuracy got reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdBoosting with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.99\n",
      "Test score: 0.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = rf_clf, learning_rate = 0.5)\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(adaboost_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(adaboost_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adaptive boosting did in fact raise the average training accuracy for the Decision Tree but the test accuracy got reduced. It is still overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.57\n",
      "Test score: 0.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = lreg_clf, learning_rate = 0.5)\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(adaboost_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(adaboost_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adaptive boosting improved our test accuracy with logistic model and has an improved model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting with LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.93\n",
      "Test score: 0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = LSVC_clf, algorithm='SAMME')\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(adaboost_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(adaboost_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adaptive boosting did in fact reduced the average test accuracy for the Decision Tree but the test accuracy got increased. Therefore, it has a poor fit with Linear SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting with KNN\n",
    "- KNeighborsClassifier does not support sample weights, so we will not be able to use Adaptive Boosting to lower the model bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting with Kernel SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.53\n",
      "Test score: 0.59\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = KSVC_clf, algorithm='SAMME')\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(adaboost_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(adaboost_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adaptive boosting has very low accuracy for Kernel SVC. So it's not a good fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.00\n",
      "Test score: 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(learning_rate = 0.05)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(gb_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(gb_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting is overfitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.00\n",
      "Test score: 0.72\n",
      "Mean Accuracy: 0.7372020434420865\n"
     ]
    }
   ],
   "source": [
    "GB = GradientBoostingClassifier()\n",
    "score = cross_val_score(estimator=GB, X=X_scaled, y=y, cv=kFold, n_jobs=-1)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(gb_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(gb_clf.score(X_test, y_test)))\n",
    "print('Mean Accuracy:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting is overfitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.00\n",
      "Test score: 0.74\n",
      "Mean Accuracy: 0.7415997720877312\n"
     ]
    }
   ],
   "source": [
    "GB2 = GradientBoostingClassifier(min_samples_leaf=9, learning_rate=0.05, n_estimators=100)\n",
    "score = cross_val_score(estimator=GB, X=X_scaled, y=y, cv=kFold, n_jobs=-1)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(gb_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(gb_clf.score(X_test, y_test)))\n",
    "print('Mean Accuracy:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is clear that, while the bagging and boosting techniques mentioned above are \n",
    "#### usually effective, most did not do much to improve the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Due to reasons like imbalanced classes, high dimensionality and lack of observations, we couldn't get an optimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100, svd_solver='auto')\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_comb_pca = np.concatenate((X_train_pca, X_test_pca), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 100)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Machine Learning Models with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classification with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [1, 2, 3, 4, 5, 7, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "param_grid={'n_neighbors':[1,2,3,4,5,7,10]}\n",
    "\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00150495, 0.0026341 , 0.00179591, 0.00280981, 0.00150194,\n",
       "        0.00160213, 0.0027267 ]),\n",
       " 'mean_score_time': array([0.10580077, 0.10715709, 0.1055243 , 0.10528398, 0.10647898,\n",
       "        0.10611115, 0.10878577]),\n",
       " 'mean_test_score': array([0.5546875 , 0.58854167, 0.6015625 , 0.58854167, 0.58333333,\n",
       "        0.56770833, 0.57291667]),\n",
       " 'mean_train_score': array([1.        , 0.69723114, 0.67579153, 0.63796436, 0.62625759,\n",
       "        0.60225821, 0.57490731]),\n",
       " 'param_n_neighbors': masked_array(data=[1, 2, 3, 4, 5, 7, 10],\n",
       "              mask=[False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_neighbors': 1},\n",
       "  {'n_neighbors': 2},\n",
       "  {'n_neighbors': 3},\n",
       "  {'n_neighbors': 4},\n",
       "  {'n_neighbors': 5},\n",
       "  {'n_neighbors': 7},\n",
       "  {'n_neighbors': 10}],\n",
       " 'rank_test_score': array([7, 2, 1, 2, 4, 6, 5]),\n",
       " 'split0_test_score': array([0.58536585, 0.58536585, 0.59756098, 0.57317073, 0.57317073,\n",
       "        0.54878049, 0.53658537]),\n",
       " 'split0_train_score': array([1.        , 0.67880795, 0.66556291, 0.62582781, 0.61589404,\n",
       "        0.61258278, 0.58609272]),\n",
       " 'split1_test_score': array([0.525 , 0.575 , 0.6   , 0.5875, 0.575 , 0.5625, 0.5625]),\n",
       " 'split1_train_score': array([1.        , 0.70065789, 0.6875    , 0.63157895, 0.625     ,\n",
       "        0.59539474, 0.56578947]),\n",
       " 'split2_test_score': array([0.51948052, 0.57142857, 0.58441558, 0.55844156, 0.55844156,\n",
       "        0.53246753, 0.55844156]),\n",
       " 'split2_train_score': array([1.        , 0.71661238, 0.68078176, 0.65472313, 0.63517915,\n",
       "        0.60586319, 0.57980456]),\n",
       " 'split3_test_score': array([0.57534247, 0.61643836, 0.60273973, 0.57534247, 0.57534247,\n",
       "        0.5890411 , 0.60273973]),\n",
       " 'split3_train_score': array([1.        , 0.70418006, 0.67524116, 0.63987138, 0.62700965,\n",
       "        0.60450161, 0.56913183]),\n",
       " 'split4_test_score': array([0.56944444, 0.59722222, 0.625     , 0.65277778, 0.63888889,\n",
       "        0.61111111, 0.61111111]),\n",
       " 'split4_train_score': array([1.        , 0.68589744, 0.66987179, 0.63782051, 0.62820513,\n",
       "        0.59294872, 0.57371795]),\n",
       " 'std_fit_time': array([0.00062268, 0.00181889, 0.00074878, 0.00182179, 0.00044927,\n",
       "        0.0008012 , 0.00214794]),\n",
       " 'std_score_time': array([0.00077554, 0.00175638, 0.00104919, 0.00145333, 0.00122663,\n",
       "        0.00138829, 0.00203179]),\n",
       " 'std_test_score': array([0.02747869, 0.01616025, 0.01288971, 0.03223586, 0.02741546,\n",
       "        0.0277653 , 0.02809087]),\n",
       " 'std_train_score': array([0.        , 0.01344269, 0.00777125, 0.00972196, 0.00621188,\n",
       "        0.00718887, 0.00730756])}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 3}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best Parameter : No. of neighbours=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6015625"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf=KNeighborsClassifier(n_neighbors=3)\n",
    "knn_clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.6770833333333334\n",
      "Test Recall score: 0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = knn_clf.predict(X_train_pca)\n",
    "y_pred_test = knn_clf.predict(X_test_pca)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN shows an improved model accuracy and fit after applying PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lreg_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "param_grid = {'C': [0.00001,0.0001,0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "grid_search = GridSearchCV(lreg_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.04935141, 0.0462153 , 0.03389812, 0.01575255, 0.01193838,\n",
       "        0.0149333 , 0.03399248, 0.04452558]),\n",
       " 'mean_score_time': array([0.00099087, 0.00040092, 0.00070515, 0.00050015, 0.00029573,\n",
       "        0.0005115 , 0.00069308, 0.00070167]),\n",
       " 'mean_test_score': array([0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.609375  ,\n",
       "        0.70572917, 0.6875    , 0.64583333]),\n",
       " 'mean_train_score': array([0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.64196531,\n",
       "        0.82880072, 0.95507739, 0.9973978 ]),\n",
       " 'param_C': masked_array(data=[1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1e-05},\n",
       "  {'C': 0.0001},\n",
       "  {'C': 0.001},\n",
       "  {'C': 0.01},\n",
       "  {'C': 0.1},\n",
       "  {'C': 1},\n",
       "  {'C': 10},\n",
       "  {'C': 100}],\n",
       " 'rank_test_score': array([5, 5, 5, 5, 4, 1, 2, 3]),\n",
       " 'split0_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.59756098,\n",
       "        0.68292683, 0.68292683, 0.63414634]),\n",
       " 'split0_train_score': array([0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.63907285,\n",
       "        0.82781457, 0.95364238, 0.99668874]),\n",
       " 'split1_test_score': array([0.5125, 0.5125, 0.5125, 0.5125, 0.6   , 0.7   , 0.6375, 0.6125]),\n",
       " 'split1_train_score': array([0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.65131579,\n",
       "        0.83881579, 0.95065789, 0.99671053]),\n",
       " 'split2_test_score': array([0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.58441558,\n",
       "        0.67532468, 0.66233766, 0.62337662]),\n",
       " 'split2_train_score': array([0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.64495114,\n",
       "        0.82410423, 0.96416938, 1.        ]),\n",
       " 'split3_test_score': array([0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.61643836,\n",
       "        0.7260274 , 0.73972603, 0.69863014]),\n",
       " 'split3_train_score': array([0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.63987138,\n",
       "        0.81993569, 0.95819936, 1.        ]),\n",
       " 'split4_test_score': array([0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.65277778,\n",
       "        0.75      , 0.72222222, 0.66666667]),\n",
       " 'split4_train_score': array([0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.63461538,\n",
       "        0.83333333, 0.94871795, 0.99358974]),\n",
       " 'std_fit_time': array([0.00323897, 0.0015483 , 0.00449685, 0.00165081, 0.00074259,\n",
       "        0.00081086, 0.00134329, 0.00060413]),\n",
       " 'std_score_time': array([1.59943153e-05, 2.00463532e-04, 2.49991975e-04, 7.54307554e-06,\n",
       "        2.42019948e-04, 1.83446636e-05, 2.35292057e-04, 2.46355139e-04]),\n",
       " 'std_test_score': array([0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02314586,\n",
       "        0.02736869, 0.03737424, 0.03119403]),\n",
       " 'std_train_score': array([0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00571172,\n",
       "        0.0066703 , 0.00555778, 0.00240912])}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameter : C=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7057291666666666"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs',C=1)\n",
    "lreg_clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.8203125\n",
      "Test Recall score: 0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = lreg_clf.predict(X_train_pca)\n",
    "y_pred_test = lreg_clf.predict(X_test_pca)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistics Regression shows an improved model accuracy and fit after applying PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=None,\n",
       "     tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "LSVC_clf = LinearSVC(multi_class='crammer_singer')\n",
    "\n",
    "param_grid = {'C': [0.00001,0.0001,0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "grid_search = GridSearchCV(LSVC_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00381737, 0.00501084, 0.01052814, 0.01514807, 0.01824827,\n",
       "        0.05203772, 0.1637548 , 0.22758698]),\n",
       " 'mean_score_time': array([0.0008018 , 0.00080309, 0.00040121, 0.00060167, 0.0008028 ,\n",
       "        0.00070238, 0.00069957, 0.00080895]),\n",
       " 'mean_test_score': array([0.61458333, 0.61458333, 0.61458333, 0.6171875 , 0.69791667,\n",
       "        0.70052083, 0.63020833, 0.59114583]),\n",
       " 'mean_train_score': array([0.6477196 , 0.64448269, 0.64448269, 0.65356614, 0.77480555,\n",
       "        0.91137569, 0.98760332, 1.        ]),\n",
       " 'param_C': masked_array(data=[1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1e-05},\n",
       "  {'C': 0.0001},\n",
       "  {'C': 0.001},\n",
       "  {'C': 0.01},\n",
       "  {'C': 0.1},\n",
       "  {'C': 1},\n",
       "  {'C': 10},\n",
       "  {'C': 100}],\n",
       " 'rank_test_score': array([5, 5, 5, 4, 2, 1, 3, 8]),\n",
       " 'split0_test_score': array([0.62195122, 0.62195122, 0.62195122, 0.62195122, 0.68292683,\n",
       "        0.65853659, 0.63414634, 0.6097561 ]),\n",
       " 'split0_train_score': array([0.63907285, 0.63576159, 0.63576159, 0.63907285, 0.78476821,\n",
       "        0.89735099, 0.98675497, 1.        ]),\n",
       " 'split1_test_score': array([0.575 , 0.575 , 0.575 , 0.5875, 0.675 , 0.7375, 0.6   , 0.55  ]),\n",
       " 'split1_train_score': array([0.64802632, 0.64473684, 0.64473684, 0.65131579, 0.77960526,\n",
       "        0.91118421, 0.98355263, 1.        ]),\n",
       " 'split2_test_score': array([0.61038961, 0.61038961, 0.61038961, 0.61038961, 0.7012987 ,\n",
       "        0.67532468, 0.63636364, 0.61038961]),\n",
       " 'split2_train_score': array([0.64169381, 0.64495114, 0.64495114, 0.65798046, 0.76872964,\n",
       "        0.91205212, 0.98697068, 1.        ]),\n",
       " 'split3_test_score': array([0.60273973, 0.60273973, 0.60273973, 0.60273973, 0.69863014,\n",
       "        0.69863014, 0.64383562, 0.63013699]),\n",
       " 'split3_train_score': array([0.65916399, 0.65273312, 0.65273312, 0.67202572, 0.76848875,\n",
       "        0.92282958, 0.9903537 , 1.        ]),\n",
       " 'split4_test_score': array([0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.73611111,\n",
       "        0.73611111, 0.63888889, 0.55555556]),\n",
       " 'split4_train_score': array([0.65064103, 0.64423077, 0.64423077, 0.6474359 , 0.7724359 ,\n",
       "        0.91346154, 0.99038462, 1.        ]),\n",
       " 'std_fit_time': array([1.17015356e-03, 3.82362751e-06, 4.48354804e-04, 2.06647876e-03,\n",
       "        2.04977088e-03, 8.88607671e-03, 3.84383493e-02, 5.16603473e-02]),\n",
       " 'std_score_time': array([0.0004009 , 0.00051232, 0.00049138, 0.00049126, 0.0004014 ,\n",
       "        0.00024567, 0.00024008, 0.00025045]),\n",
       " 'std_test_score': array([0.02961142, 0.02961142, 0.02961142, 0.02639022, 0.02081613,\n",
       "        0.03204328, 0.01582572, 0.03205367]),\n",
       " 'std_train_score': array([0.00708067, 0.0053754 , 0.0053754 , 0.01107196, 0.00639884,\n",
       "        0.00815756, 0.00256235, 0.        ])}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameter : C=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7005208333333334"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=None,\n",
       "     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSVC_clf = LinearSVC(multi_class='crammer_singer', C=1)\n",
    "LSVC_clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.90625\n",
      "Test Recall score: 0.6911764705882353\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = LSVC_clf.predict(X_train_pca)\n",
    "y_pred_test = LSVC_clf.predict(X_test_pca)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVM still overfits the model after applying PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernalised SVM with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 1, 10], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "KSVC_clf = svm.SVC(kernel='rbf')\n",
    "\n",
    "param_grid = {'C': [0.0001,0.001,0.01,0.1,1,10],\n",
    "          'gamma': [0.0001,0.001,0.01,0.1,1,10]}\n",
    "\n",
    "grid_search = GridSearchCV(KSVC_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01242585, 0.01113987, 0.01113796, 0.01152434, 0.01132798,\n",
       "        0.0114357 , 0.01123881, 0.01110315, 0.01221323, 0.01363592,\n",
       "        0.01293464, 0.01434097, 0.01185708, 0.01222363, 0.01213918,\n",
       "        0.017243  , 0.03218026, 0.04190698, 0.01202068, 0.01293826,\n",
       "        0.01580896, 0.02025504, 0.04337721, 0.05393009, 0.01264277,\n",
       "        0.01595249, 0.01654754, 0.01974859, 0.04744077, 0.05633597,\n",
       "        0.01525431, 0.01695333, 0.01716261, 0.02118173, 0.05100412,\n",
       "        0.06006169]),\n",
       " 'mean_score_time': array([0.00271959, 0.00281529, 0.00240536, 0.00271316, 0.00251427,\n",
       "        0.00249405, 0.00261116, 0.00253105, 0.00300169, 0.00300822,\n",
       "        0.00280786, 0.00300584, 0.00268836, 0.0030077 , 0.00271163,\n",
       "        0.00301394, 0.00340357, 0.00357475, 0.00272193, 0.00281458,\n",
       "        0.00314631, 0.00332031, 0.00342102, 0.00350986, 0.00270729,\n",
       "        0.00322375, 0.0027072 , 0.00310864, 0.00320878, 0.00341716,\n",
       "        0.00280895, 0.00290127, 0.0032012 , 0.00310936, 0.00364909,\n",
       "        0.00381403]),\n",
       " 'mean_test_score': array([0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.65104167, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.69791667, 0.70833333, 0.5390625 ,\n",
       "        0.53385417]),\n",
       " 'mean_train_score': array([0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.5371777 , 0.73636858, 0.94210098, 1.        ,\n",
       "        0.53393884, 0.542369  , 0.75719163, 0.96878148, 1.        ,\n",
       "        1.        ]),\n",
       " 'param_C': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1, 1,\n",
       "                    1, 1, 1, 1, 10, 10, 10, 10, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_gamma': masked_array(data=[0.0001, 0.001, 0.01, 0.1, 1, 10, 0.0001, 0.001, 0.01,\n",
       "                    0.1, 1, 10, 0.0001, 0.001, 0.01, 0.1, 1, 10, 0.0001,\n",
       "                    0.001, 0.01, 0.1, 1, 10, 0.0001, 0.001, 0.01, 0.1, 1,\n",
       "                    10, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.0001, 'gamma': 0.0001},\n",
       "  {'C': 0.0001, 'gamma': 0.001},\n",
       "  {'C': 0.0001, 'gamma': 0.01},\n",
       "  {'C': 0.0001, 'gamma': 0.1},\n",
       "  {'C': 0.0001, 'gamma': 1},\n",
       "  {'C': 0.0001, 'gamma': 10},\n",
       "  {'C': 0.001, 'gamma': 0.0001},\n",
       "  {'C': 0.001, 'gamma': 0.001},\n",
       "  {'C': 0.001, 'gamma': 0.01},\n",
       "  {'C': 0.001, 'gamma': 0.1},\n",
       "  {'C': 0.001, 'gamma': 1},\n",
       "  {'C': 0.001, 'gamma': 10},\n",
       "  {'C': 0.01, 'gamma': 0.0001},\n",
       "  {'C': 0.01, 'gamma': 0.001},\n",
       "  {'C': 0.01, 'gamma': 0.01},\n",
       "  {'C': 0.01, 'gamma': 0.1},\n",
       "  {'C': 0.01, 'gamma': 1},\n",
       "  {'C': 0.01, 'gamma': 10},\n",
       "  {'C': 0.1, 'gamma': 0.0001},\n",
       "  {'C': 0.1, 'gamma': 0.001},\n",
       "  {'C': 0.1, 'gamma': 0.01},\n",
       "  {'C': 0.1, 'gamma': 0.1},\n",
       "  {'C': 0.1, 'gamma': 1},\n",
       "  {'C': 0.1, 'gamma': 10},\n",
       "  {'C': 1, 'gamma': 0.0001},\n",
       "  {'C': 1, 'gamma': 0.001},\n",
       "  {'C': 1, 'gamma': 0.01},\n",
       "  {'C': 1, 'gamma': 0.1},\n",
       "  {'C': 1, 'gamma': 1},\n",
       "  {'C': 1, 'gamma': 10},\n",
       "  {'C': 10, 'gamma': 0.0001},\n",
       "  {'C': 10, 'gamma': 0.001},\n",
       "  {'C': 10, 'gamma': 0.01},\n",
       "  {'C': 10, 'gamma': 0.1},\n",
       "  {'C': 10, 'gamma': 1},\n",
       "  {'C': 10, 'gamma': 10}],\n",
       " 'rank_test_score': array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 2, 1, 4, 5]),\n",
       " 'split0_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.6097561 , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.69512195, 0.69512195, 0.51219512,\n",
       "        0.5       ]),\n",
       " 'split0_train_score': array([0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54635762, 0.73509934, 0.94039735, 1.        ,\n",
       "        0.54304636, 0.54966887, 0.7615894 , 0.96688742, 1.        ,\n",
       "        1.        ]),\n",
       " 'split1_test_score': array([0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.5125, 0.5125, 0.5125, 0.6375, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.6875, 0.725 , 0.5   , 0.5125]),\n",
       " 'split1_train_score': array([0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.74342105, 0.94736842, 1.        ,\n",
       "        0.53947368, 0.54276316, 0.75657895, 0.97368421, 1.        ,\n",
       "        1.        ]),\n",
       " 'split2_test_score': array([0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.62337662, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.66233766, 0.7012987 , 0.54545455,\n",
       "        0.53246753]),\n",
       " 'split2_train_score': array([0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53745928, 0.73941368, 0.95114007, 1.        ,\n",
       "        0.53420195, 0.54397394, 0.75895765, 0.97394137, 1.        ,\n",
       "        1.        ]),\n",
       " 'split3_test_score': array([0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.65753425, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.69863014, 0.68493151, 0.57534247,\n",
       "        0.56164384]),\n",
       " 'split3_train_score': array([0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.53054662, 0.73954984, 0.93890675, 1.        ,\n",
       "        0.52733119, 0.53697749, 0.75562701, 0.97106109, 1.        ,\n",
       "        1.        ]),\n",
       " 'split4_test_score': array([0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.73611111, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.75      , 0.73611111, 0.56944444,\n",
       "        0.56944444]),\n",
       " 'split4_train_score': array([0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.53205128, 0.72435897, 0.93269231, 1.        ,\n",
       "        0.52564103, 0.53846154, 0.75320513, 0.95833333, 1.        ,\n",
       "        1.        ]),\n",
       " 'std_fit_time': array([0.00049802, 0.00019795, 0.00038164, 0.00063759, 0.00038222,\n",
       "        0.00068852, 0.00050147, 0.00047244, 0.00105128, 0.00146668,\n",
       "        0.00058441, 0.00037938, 0.00051533, 0.00093046, 0.00046531,\n",
       "        0.00062109, 0.00133862, 0.00170836, 0.00057073, 0.00055086,\n",
       "        0.00072905, 0.00053473, 0.00134296, 0.00101736, 0.00057554,\n",
       "        0.00065924, 0.00054247, 0.00098661, 0.00134992, 0.00172367,\n",
       "        0.00060679, 0.00097811, 0.00048518, 0.00060648, 0.00084613,\n",
       "        0.00359758]),\n",
       " 'std_score_time': array([4.02573662e-04, 5.23524561e-04, 1.99416207e-04, 2.43405982e-04,\n",
       "        3.05343917e-04, 3.02734835e-04, 2.16596266e-04, 2.98963095e-04,\n",
       "        3.17596923e-04, 3.17109638e-04, 2.45185429e-04, 3.17521334e-04,\n",
       "        2.13113053e-04, 3.18852896e-04, 2.50193172e-04, 3.98470420e-05,\n",
       "        2.00978836e-04, 3.69982268e-04, 2.44184095e-04, 3.81009074e-04,\n",
       "        3.74367844e-04, 3.93439163e-04, 2.07354676e-04, 3.17034378e-04,\n",
       "        2.45729849e-04, 2.64821277e-04, 2.45709908e-04, 2.00677166e-04,\n",
       "        2.45748872e-04, 3.23962042e-04, 2.45969163e-04, 1.99687333e-04,\n",
       "        2.27525907e-04, 2.25614563e-04, 3.69005561e-04, 4.13310236e-04]),\n",
       " 'std_test_score': array([0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.04382431, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02805047, 0.01881092, 0.03005872,\n",
       "        0.02691538]),\n",
       " 'std_train_score': array([0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00565461, 0.00655701, 0.00649738, 0.        ,\n",
       "        0.00672555, 0.00448009, 0.00286966, 0.00580705, 0.        ,\n",
       "        0.        ])}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.1}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameters : C=0.0001, gamma=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7083333333333334"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "KSVC_clf = svm.SVC(kernel='rbf',C=10,gamma=0.1)\n",
    "KSVC_clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.9661458333333334\n",
      "Test Recall score: 0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = KSVC_clf.predict(X_train_pca)\n",
    "y_pred_test = KSVC_clf.predict(X_test_pca)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernalized SVM still overfits the model after applying PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [2, 3, 4, 5, 6, 10, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "param_grid = {'max_depth': [2, 3, 4, 5,6, 10, 20]}\n",
    "\n",
    "grid_search = GridSearchCV(dt_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00621614, 0.00882773, 0.0105288 , 0.0134357 , 0.01483927,\n",
       "        0.02005253, 0.02506371]),\n",
       " 'mean_score_time': array([0.0008029 , 0.00020027, 0.00090156, 0.00050211, 0.00040164,\n",
       "        0.0006042 , 0.00060225]),\n",
       " 'mean_test_score': array([0.53385417, 0.52864583, 0.52604167, 0.53385417, 0.52083333,\n",
       "        0.47395833, 0.40885417]),\n",
       " 'mean_train_score': array([0.58802314, 0.63098482, 0.70465248, 0.77037599, 0.82632373,\n",
       "        0.94215768, 1.        ]),\n",
       " 'param_max_depth': masked_array(data=[2, 3, 4, 5, 6, 10, 20],\n",
       "              mask=[False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 2},\n",
       "  {'max_depth': 3},\n",
       "  {'max_depth': 4},\n",
       "  {'max_depth': 5},\n",
       "  {'max_depth': 6},\n",
       "  {'max_depth': 10},\n",
       "  {'max_depth': 20}],\n",
       " 'rank_test_score': array([1, 3, 4, 1, 5, 6, 7]),\n",
       " 'split0_test_score': array([0.45121951, 0.43902439, 0.42682927, 0.43902439, 0.42682927,\n",
       "        0.3902439 , 0.34146341]),\n",
       " 'split0_train_score': array([0.60264901, 0.64569536, 0.73178808, 0.79139073, 0.83774834,\n",
       "        0.96357616, 1.        ]),\n",
       " 'split1_test_score': array([0.55  , 0.5625, 0.5625, 0.5875, 0.5375, 0.5125, 0.45  ]),\n",
       " 'split1_train_score': array([0.59539474, 0.64473684, 0.72368421, 0.78947368, 0.84539474,\n",
       "        0.94407895, 1.        ]),\n",
       " 'split2_test_score': array([0.54545455, 0.54545455, 0.51948052, 0.55844156, 0.53246753,\n",
       "        0.48051948, 0.36363636]),\n",
       " 'split2_train_score': array([0.58631922, 0.61889251, 0.69055375, 0.75895765, 0.82410423,\n",
       "        0.93159609, 1.        ]),\n",
       " 'split3_test_score': array([0.5890411 , 0.54794521, 0.53424658, 0.52054795, 0.56164384,\n",
       "        0.52054795, 0.43835616]),\n",
       " 'split3_train_score': array([0.59485531, 0.62700965, 0.6977492 , 0.76205788, 0.80385852,\n",
       "        0.91961415, 1.        ]),\n",
       " 'split4_test_score': array([0.54166667, 0.55555556, 0.59722222, 0.56944444, 0.55555556,\n",
       "        0.47222222, 0.45833333]),\n",
       " 'split4_train_score': array([0.56089744, 0.61858974, 0.67948718, 0.75      , 0.82051282,\n",
       "        0.95192308, 1.        ]),\n",
       " 'std_fit_time': array([0.00040169, 0.00040379, 0.00045429, 0.00037591, 0.00051227,\n",
       "        0.00100243, 0.00426662]),\n",
       " 'std_score_time': array([0.00040145, 0.00040054, 0.00019973, 0.00044905, 0.00037586,\n",
       "        0.00020382, 0.00020075]),\n",
       " 'std_test_score': array([0.04615068, 0.04708689, 0.05788145, 0.05396529, 0.05014682,\n",
       "        0.04719989, 0.04854891]),\n",
       " 'std_train_score': array([0.01451649, 0.01200985, 0.01989105, 0.01685878, 0.01440558,\n",
       "        0.01534642, 0.        ])}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 2}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameter : max_depth = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5338541666666666"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(max_depth=4)\n",
    "dt_clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.703125\n",
      "Test Recall score: 0.5441176470588235\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = dt_clf.predict(X_train_pca)\n",
    "y_pred_test = dt_clf.predict(X_test_pca)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that decision tree performs really bad with PCA. This may be due to information lose due to dimentionality reduction.\n",
    "- Also model is also overfitting as there is difference between train and test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
       "            oob_score=False, random_state=10, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_features': [25, 50, 75, 100], 'max_depth': [6, 8, 10, 12, 14], 'max_leaf_nodes': [20, 22, 30, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=300, criterion='gini',n_jobs= -1,random_state=10)\n",
    "\n",
    "param_grid = {'max_features': [25,50,75,100],\n",
    "          'max_depth': [6,8,10,12,14],\n",
    "           'max_leaf_nodes':[20,22,30,50]}\n",
    "\n",
    "grid_search = GridSearchCV(rf_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.47591825, 0.45649786, 0.48133321, 0.49413848, 0.8034472 ,\n",
       "        0.70173945, 0.69675908, 0.69867725, 0.91818199, 0.91655087,\n",
       "        0.93591719, 0.9408114 , 1.19592323, 1.35750651, 1.27654238,\n",
       "        1.39682217, 0.61368871, 0.64370117, 0.61931663, 0.60074053,\n",
       "        0.82515993, 0.83641367, 0.83594499, 1.01602736, 1.17141442,\n",
       "        1.17759404, 1.23317142, 1.28809524, 1.38579321, 1.45236483,\n",
       "        1.45978646, 1.49257674, 0.50639396, 0.57564278, 0.61606793,\n",
       "        0.67804885, 0.97903004, 0.81321459, 0.87057991, 0.91873593,\n",
       "        1.26251774, 1.17568493, 1.44062138, 1.59124384, 1.61125503,\n",
       "        1.42072124, 1.53226852, 1.66985369, 0.53028164, 0.53260016,\n",
       "        0.54696441, 0.61248631, 0.82627439, 0.85944004, 0.95739627,\n",
       "        0.977669  , 1.18965182, 1.25807791, 1.27347813, 1.32724929,\n",
       "        1.37723961, 1.43330655, 1.55271797, 1.77571683, 0.50543461,\n",
       "        0.52532287, 0.61474838, 0.6427238 , 0.89739299, 0.91250215,\n",
       "        0.92872071, 1.04022484, 1.14632974, 1.1440928 , 1.32023582,\n",
       "        1.4278605 , 1.49056082, 1.62660704, 1.74177256, 1.89425087]),\n",
       " 'mean_score_time': array([0.11003151, 0.1102488 , 0.10662165, 0.10696793, 0.10572577,\n",
       "        0.13018546, 0.10881028, 0.10909653, 0.1091167 , 0.10717506,\n",
       "        0.10689087, 0.10811415, 0.10903416, 0.10726314, 0.107623  ,\n",
       "        0.10395513, 0.10457869, 0.10490866, 0.10539465, 0.10460196,\n",
       "        0.10463843, 0.10529065, 0.10712113, 0.10469952, 0.10450182,\n",
       "        0.10460081, 0.10425463, 0.10495291, 0.10752077, 0.10750341,\n",
       "        0.10533257, 0.10796423, 0.10808358, 0.1083487 , 0.10733294,\n",
       "        0.10506406, 0.10489936, 0.1108427 , 0.12914615, 0.10725226,\n",
       "        0.10662103, 0.10793238, 0.10613508, 0.10602641, 0.10663295,\n",
       "        0.10835347, 0.10800595, 0.10696311, 0.10799117, 0.10631971,\n",
       "        0.10807738, 0.10895162, 0.10950518, 0.1068625 , 0.1063024 ,\n",
       "        0.10638165, 0.10763593, 0.1095542 , 0.10708833, 0.1095901 ,\n",
       "        0.1075582 , 0.10875239, 0.1086638 , 0.10939498, 0.10773335,\n",
       "        0.12728815, 0.11071477, 0.10758801, 0.10741873, 0.10978794,\n",
       "        0.10908198, 0.10812554, 0.10988822, 0.10894156, 0.10795164,\n",
       "        0.12997952, 0.12702646, 0.10742731, 0.10782771, 0.10672932]),\n",
       " 'mean_test_score': array([0.578125  , 0.58072917, 0.58072917, 0.578125  , 0.59635417,\n",
       "        0.60416667, 0.60416667, 0.60677083, 0.60677083, 0.609375  ,\n",
       "        0.609375  , 0.61458333, 0.6015625 , 0.60416667, 0.60416667,\n",
       "        0.6015625 , 0.58333333, 0.58333333, 0.58333333, 0.59635417,\n",
       "        0.59895833, 0.6015625 , 0.61979167, 0.6171875 , 0.60677083,\n",
       "        0.60677083, 0.61197917, 0.6171875 , 0.60677083, 0.60677083,\n",
       "        0.60677083, 0.609375  , 0.58333333, 0.58333333, 0.58333333,\n",
       "        0.6015625 , 0.59895833, 0.60677083, 0.6171875 , 0.61979167,\n",
       "        0.60677083, 0.609375  , 0.61979167, 0.61979167, 0.60677083,\n",
       "        0.60677083, 0.609375  , 0.61197917, 0.58333333, 0.5859375 ,\n",
       "        0.58072917, 0.60677083, 0.59895833, 0.60677083, 0.6171875 ,\n",
       "        0.625     , 0.60677083, 0.609375  , 0.61979167, 0.62239583,\n",
       "        0.60677083, 0.60677083, 0.609375  , 0.61979167, 0.58333333,\n",
       "        0.5859375 , 0.58072917, 0.60677083, 0.59895833, 0.60677083,\n",
       "        0.6171875 , 0.625     , 0.60677083, 0.609375  , 0.61979167,\n",
       "        0.625     , 0.60677083, 0.60677083, 0.609375  , 0.61979167]),\n",
       " 'mean_train_score': array([0.81643168, 0.83141977, 0.88880663, 0.91478909, 0.83280565,\n",
       "        0.85429097, 0.90307967, 0.92461112, 0.84061614, 0.86147074,\n",
       "        0.90701913, 0.92595043, 0.8477646 , 0.86671725, 0.9096803 ,\n",
       "        0.92599897, 0.82101121, 0.8418664 , 0.91479483, 0.95320976,\n",
       "        0.84129503, 0.86860637, 0.92978096, 0.95714931, 0.84643367,\n",
       "        0.87378894, 0.93242331, 0.95911484, 0.85496561, 0.87835438,\n",
       "        0.93761026, 0.96038816, 0.82035572, 0.84445379, 0.91998383,\n",
       "        0.96622461, 0.83994965, 0.86726063, 0.93240817, 0.97338547,\n",
       "        0.84643367, 0.8763676 , 0.93692918, 0.97403292, 0.85562351,\n",
       "        0.87967418, 0.94022496, 0.97532748, 0.82035572, 0.84445379,\n",
       "        0.91998589, 0.97404737, 0.84060112, 0.86726063, 0.93239336,\n",
       "        0.97924706, 0.84643367, 0.87508349, 0.93823016, 0.98313772,\n",
       "        0.85562351, 0.87967418, 0.94022496, 0.98443871, 0.82035572,\n",
       "        0.84445379, 0.92127839, 0.97988819, 0.84060112, 0.86726063,\n",
       "        0.93239336, 0.98507744, 0.84643367, 0.87508349, 0.93823016,\n",
       "        0.98832272, 0.85562351, 0.87967418, 0.94022496, 0.98835234]),\n",
       " 'param_max_depth': masked_array(data=[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8,\n",
       "                    8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "                    14, 14, 14],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=[25, 25, 25, 25, 50, 50, 50, 50, 75, 75, 75, 75, 100,\n",
       "                    100, 100, 100, 25, 25, 25, 25, 50, 50, 50, 50, 75, 75,\n",
       "                    75, 75, 100, 100, 100, 100, 25, 25, 25, 25, 50, 50, 50,\n",
       "                    50, 75, 75, 75, 75, 100, 100, 100, 100, 25, 25, 25, 25,\n",
       "                    50, 50, 50, 50, 75, 75, 75, 75, 100, 100, 100, 100, 25,\n",
       "                    25, 25, 25, 50, 50, 50, 50, 75, 75, 75, 75, 100, 100,\n",
       "                    100, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_leaf_nodes': masked_array(data=[20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22,\n",
       "                    30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50,\n",
       "                    20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22,\n",
       "                    30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50,\n",
       "                    20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22,\n",
       "                    30, 50, 20, 22, 30, 50, 20, 22, 30, 50],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 6, 'max_features': 25, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 6, 'max_features': 25, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 6, 'max_features': 25, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 6, 'max_features': 25, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 6, 'max_features': 50, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 6, 'max_features': 50, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 6, 'max_features': 50, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 6, 'max_features': 50, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 6, 'max_features': 75, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 6, 'max_features': 75, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 6, 'max_features': 75, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 6, 'max_features': 75, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 6, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 6, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 6, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 6, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 8, 'max_features': 25, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 8, 'max_features': 25, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 8, 'max_features': 25, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 8, 'max_features': 25, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 8, 'max_features': 50, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 8, 'max_features': 50, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 8, 'max_features': 50, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 8, 'max_features': 50, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 8, 'max_features': 75, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 8, 'max_features': 75, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 8, 'max_features': 75, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 8, 'max_features': 75, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 8, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 8, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 8, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 8, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 10, 'max_features': 25, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 10, 'max_features': 25, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 10, 'max_features': 25, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 10, 'max_features': 25, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 10, 'max_features': 50, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 10, 'max_features': 50, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 10, 'max_features': 50, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 10, 'max_features': 50, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 10, 'max_features': 75, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 10, 'max_features': 75, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 10, 'max_features': 75, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 10, 'max_features': 75, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 12, 'max_features': 25, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 12, 'max_features': 25, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 12, 'max_features': 25, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 12, 'max_features': 25, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 12, 'max_features': 50, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 12, 'max_features': 50, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 12, 'max_features': 50, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 12, 'max_features': 50, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 12, 'max_features': 75, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 12, 'max_features': 75, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 12, 'max_features': 75, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 12, 'max_features': 75, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 12, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 12, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 12, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 12, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 14, 'max_features': 25, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 14, 'max_features': 25, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 14, 'max_features': 25, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 14, 'max_features': 25, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 14, 'max_features': 50, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 14, 'max_features': 50, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 14, 'max_features': 50, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 14, 'max_features': 50, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 14, 'max_features': 75, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 14, 'max_features': 75, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 14, 'max_features': 75, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 14, 'max_features': 75, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 14, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 14, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 14, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 14, 'max_features': 100, 'max_leaf_nodes': 50}],\n",
       " 'rank_test_score': array([79, 75, 75, 79, 63, 51, 51, 30, 30, 21, 21, 18, 55, 51, 51, 55, 67,\n",
       "        67, 67, 63, 59, 55,  5, 13, 30, 30, 19, 13, 30, 30, 30, 21, 67, 67,\n",
       "        67, 55, 59, 30, 13,  5, 30, 21,  5,  5, 30, 30, 21, 19, 67, 65, 75,\n",
       "        30, 59, 30, 13,  1, 30, 21,  5,  4, 30, 30, 21,  5, 67, 65, 75, 30,\n",
       "        59, 30, 13,  1, 30, 21,  5,  1, 30, 30, 21,  5]),\n",
       " 'split0_test_score': array([0.53658537, 0.54878049, 0.54878049, 0.54878049, 0.54878049,\n",
       "        0.56097561, 0.56097561, 0.56097561, 0.56097561, 0.56097561,\n",
       "        0.54878049, 0.57317073, 0.54878049, 0.54878049, 0.54878049,\n",
       "        0.56097561, 0.54878049, 0.54878049, 0.56097561, 0.56097561,\n",
       "        0.56097561, 0.54878049, 0.58536585, 0.58536585, 0.56097561,\n",
       "        0.54878049, 0.54878049, 0.56097561, 0.54878049, 0.54878049,\n",
       "        0.54878049, 0.56097561, 0.54878049, 0.54878049, 0.56097561,\n",
       "        0.57317073, 0.54878049, 0.56097561, 0.58536585, 0.57317073,\n",
       "        0.56097561, 0.54878049, 0.56097561, 0.56097561, 0.54878049,\n",
       "        0.54878049, 0.54878049, 0.54878049, 0.54878049, 0.54878049,\n",
       "        0.56097561, 0.58536585, 0.54878049, 0.56097561, 0.58536585,\n",
       "        0.57317073, 0.56097561, 0.54878049, 0.56097561, 0.57317073,\n",
       "        0.54878049, 0.54878049, 0.54878049, 0.54878049, 0.54878049,\n",
       "        0.54878049, 0.56097561, 0.57317073, 0.54878049, 0.56097561,\n",
       "        0.58536585, 0.57317073, 0.56097561, 0.54878049, 0.56097561,\n",
       "        0.57317073, 0.54878049, 0.54878049, 0.54878049, 0.54878049]),\n",
       " 'split0_train_score': array([0.82450331, 0.83774834, 0.90728477, 0.93046358, 0.85099338,\n",
       "        0.87748344, 0.91721854, 0.94370861, 0.86423841, 0.88741722,\n",
       "        0.92384106, 0.95033113, 0.87086093, 0.89403974, 0.93046358,\n",
       "        0.95033113, 0.82781457, 0.85761589, 0.92384106, 0.9602649 ,\n",
       "        0.86092715, 0.88741722, 0.94039735, 0.97350993, 0.86423841,\n",
       "        0.89072848, 0.94701987, 0.98013245, 0.8807947 , 0.89403974,\n",
       "        0.94701987, 0.97682119, 0.83112583, 0.86092715, 0.92384106,\n",
       "        0.98013245, 0.85430464, 0.8807947 , 0.94039735, 0.98675497,\n",
       "        0.86423841, 0.89072848, 0.94701987, 0.98675497, 0.8807947 ,\n",
       "        0.89403974, 0.95033113, 0.98675497, 0.83112583, 0.86092715,\n",
       "        0.92384106, 0.98675497, 0.85430464, 0.8807947 , 0.94039735,\n",
       "        0.99006623, 0.86423841, 0.89072848, 0.94701987, 0.99668874,\n",
       "        0.8807947 , 0.89403974, 0.95033113, 0.99668874, 0.83112583,\n",
       "        0.86092715, 0.92384106, 0.99006623, 0.85430464, 0.8807947 ,\n",
       "        0.94039735, 0.99337748, 0.86423841, 0.89072848, 0.94701987,\n",
       "        0.99668874, 0.8807947 , 0.89403974, 0.95033113, 0.99668874]),\n",
       " 'split1_test_score': array([0.575 , 0.575 , 0.575 , 0.575 , 0.6   , 0.6   , 0.6   , 0.6125,\n",
       "        0.6   , 0.6125, 0.6125, 0.6125, 0.6   , 0.6   , 0.6125, 0.6   ,\n",
       "        0.575 , 0.575 , 0.575 , 0.6   , 0.6   , 0.6125, 0.6125, 0.6125,\n",
       "        0.6   , 0.6   , 0.6125, 0.625 , 0.6   , 0.6   , 0.6125, 0.6125,\n",
       "        0.575 , 0.5875, 0.5875, 0.6125, 0.6   , 0.6125, 0.6125, 0.6125,\n",
       "        0.6   , 0.6125, 0.625 , 0.625 , 0.6   , 0.6   , 0.6125, 0.6125,\n",
       "        0.575 , 0.5875, 0.575 , 0.6125, 0.6   , 0.6125, 0.6125, 0.6125,\n",
       "        0.6   , 0.6125, 0.625 , 0.625 , 0.6   , 0.6   , 0.6125, 0.6375,\n",
       "        0.575 , 0.5875, 0.575 , 0.6125, 0.6   , 0.6125, 0.6125, 0.6125,\n",
       "        0.6   , 0.6125, 0.625 , 0.625 , 0.6   , 0.6   , 0.6125, 0.625 ]),\n",
       " 'split1_train_score': array([0.8125    , 0.83223684, 0.89144737, 0.91118421, 0.83552632,\n",
       "        0.85526316, 0.90131579, 0.92434211, 0.83223684, 0.85855263,\n",
       "        0.91447368, 0.92763158, 0.84210526, 0.87171053, 0.92105263,\n",
       "        0.9375    , 0.81578947, 0.83881579, 0.91447368, 0.95723684,\n",
       "        0.84539474, 0.875     , 0.93092105, 0.96052632, 0.83881579,\n",
       "        0.87171053, 0.9375    , 0.96052632, 0.85197368, 0.88486842,\n",
       "        0.94407895, 0.96052632, 0.8125    , 0.83881579, 0.92105263,\n",
       "        0.96381579, 0.84539474, 0.86842105, 0.94078947, 0.97039474,\n",
       "        0.83881579, 0.87171053, 0.9375    , 0.97368421, 0.85526316,\n",
       "        0.88815789, 0.94736842, 0.97368421, 0.8125    , 0.83881579,\n",
       "        0.92105263, 0.97039474, 0.84539474, 0.86842105, 0.9375    ,\n",
       "        0.97368421, 0.83881579, 0.87171053, 0.94078947, 0.97368421,\n",
       "        0.85526316, 0.88815789, 0.94736842, 0.97697368, 0.8125    ,\n",
       "        0.83881579, 0.92105263, 0.97697368, 0.84539474, 0.86842105,\n",
       "        0.9375    , 0.98026316, 0.83881579, 0.87171053, 0.94078947,\n",
       "        0.98355263, 0.85526316, 0.88815789, 0.94736842, 0.99013158]),\n",
       " 'split2_test_score': array([0.54545455, 0.54545455, 0.54545455, 0.54545455, 0.61038961,\n",
       "        0.63636364, 0.62337662, 0.62337662, 0.62337662, 0.62337662,\n",
       "        0.62337662, 0.62337662, 0.61038961, 0.61038961, 0.61038961,\n",
       "        0.61038961, 0.54545455, 0.54545455, 0.54545455, 0.57142857,\n",
       "        0.61038961, 0.62337662, 0.63636364, 0.63636364, 0.62337662,\n",
       "        0.62337662, 0.62337662, 0.62337662, 0.62337662, 0.61038961,\n",
       "        0.61038961, 0.61038961, 0.54545455, 0.54545455, 0.54545455,\n",
       "        0.58441558, 0.62337662, 0.62337662, 0.62337662, 0.62337662,\n",
       "        0.62337662, 0.62337662, 0.62337662, 0.61038961, 0.62337662,\n",
       "        0.61038961, 0.61038961, 0.61038961, 0.54545455, 0.54545455,\n",
       "        0.54545455, 0.58441558, 0.62337662, 0.62337662, 0.62337662,\n",
       "        0.63636364, 0.62337662, 0.62337662, 0.62337662, 0.61038961,\n",
       "        0.62337662, 0.61038961, 0.61038961, 0.61038961, 0.54545455,\n",
       "        0.54545455, 0.54545455, 0.58441558, 0.62337662, 0.62337662,\n",
       "        0.62337662, 0.63636364, 0.62337662, 0.62337662, 0.62337662,\n",
       "        0.62337662, 0.62337662, 0.61038961, 0.61038961, 0.61038961]),\n",
       " 'split2_train_score': array([0.81433225, 0.83061889, 0.89250814, 0.91530945, 0.83061889,\n",
       "        0.84690554, 0.90553746, 0.93159609, 0.84690554, 0.85993485,\n",
       "        0.90228013, 0.93159609, 0.84690554, 0.86319218, 0.90879479,\n",
       "        0.93159609, 0.83061889, 0.83713355, 0.92508143, 0.96091205,\n",
       "        0.84364821, 0.85993485, 0.93811075, 0.95765472, 0.85016287,\n",
       "        0.87296417, 0.93811075, 0.95765472, 0.85667752, 0.86970684,\n",
       "        0.94462541, 0.96091205, 0.82410423, 0.83713355, 0.93159609,\n",
       "        0.96742671, 0.83713355, 0.86319218, 0.94136808, 0.9771987 ,\n",
       "        0.85016287, 0.8762215 , 0.94136808, 0.97394137, 0.85667752,\n",
       "        0.8762215 , 0.94788274, 0.9771987 , 0.82410423, 0.83713355,\n",
       "        0.93159609, 0.98371336, 0.84039088, 0.86319218, 0.94136808,\n",
       "        0.99348534, 0.85016287, 0.8762215 , 0.94136808, 0.99348534,\n",
       "        0.85667752, 0.8762215 , 0.94788274, 0.99348534, 0.82410423,\n",
       "        0.83713355, 0.93485342, 0.98697068, 0.84039088, 0.86319218,\n",
       "        0.94136808, 0.99348534, 0.85016287, 0.8762215 , 0.94136808,\n",
       "        0.99348534, 0.85667752, 0.8762215 , 0.94788274, 0.99348534]),\n",
       " 'split3_test_score': array([0.61643836, 0.61643836, 0.61643836, 0.61643836, 0.5890411 ,\n",
       "        0.5890411 , 0.60273973, 0.60273973, 0.5890411 , 0.5890411 ,\n",
       "        0.5890411 , 0.60273973, 0.60273973, 0.61643836, 0.60273973,\n",
       "        0.60273973, 0.61643836, 0.61643836, 0.60273973, 0.61643836,\n",
       "        0.5890411 , 0.5890411 , 0.61643836, 0.60273973, 0.5890411 ,\n",
       "        0.60273973, 0.61643836, 0.61643836, 0.60273973, 0.61643836,\n",
       "        0.60273973, 0.61643836, 0.61643836, 0.60273973, 0.60273973,\n",
       "        0.60273973, 0.5890411 , 0.60273973, 0.61643836, 0.61643836,\n",
       "        0.5890411 , 0.60273973, 0.63013699, 0.63013699, 0.60273973,\n",
       "        0.61643836, 0.60273973, 0.61643836, 0.61643836, 0.61643836,\n",
       "        0.60273973, 0.61643836, 0.5890411 , 0.60273973, 0.61643836,\n",
       "        0.63013699, 0.5890411 , 0.60273973, 0.63013699, 0.63013699,\n",
       "        0.60273973, 0.61643836, 0.60273973, 0.63013699, 0.61643836,\n",
       "        0.61643836, 0.60273973, 0.61643836, 0.5890411 , 0.60273973,\n",
       "        0.61643836, 0.63013699, 0.5890411 , 0.60273973, 0.63013699,\n",
       "        0.63013699, 0.60273973, 0.61643836, 0.60273973, 0.64383562]),\n",
       " 'split3_train_score': array([0.81672026, 0.82636656, 0.87138264, 0.90032154, 0.82958199,\n",
       "        0.84244373, 0.89389068, 0.90353698, 0.82636656, 0.85209003,\n",
       "        0.88424437, 0.90032154, 0.83279743, 0.8488746 , 0.88424437,\n",
       "        0.89710611, 0.81993569, 0.8392283 , 0.90032154, 0.94212219,\n",
       "        0.82958199, 0.85209003, 0.91639871, 0.94533762, 0.83279743,\n",
       "        0.86495177, 0.91961415, 0.94533762, 0.8392283 , 0.86495177,\n",
       "        0.92604502, 0.94855305, 0.82315113, 0.84244373, 0.90996785,\n",
       "        0.96141479, 0.82958199, 0.85530547, 0.91961415, 0.96463023,\n",
       "        0.83279743, 0.87138264, 0.93247588, 0.96463023, 0.8392283 ,\n",
       "        0.86495177, 0.92926045, 0.96784566, 0.82315113, 0.84244373,\n",
       "        0.91318328, 0.96463023, 0.82958199, 0.85530547, 0.92282958,\n",
       "        0.96784566, 0.83279743, 0.8681672 , 0.93569132, 0.97106109,\n",
       "        0.8392283 , 0.86495177, 0.92926045, 0.97427653, 0.82315113,\n",
       "        0.84244373, 0.91318328, 0.97427653, 0.82958199, 0.85530547,\n",
       "        0.92282958, 0.97749196, 0.83279743, 0.8681672 , 0.93569132,\n",
       "        0.9807074 , 0.8392283 , 0.86495177, 0.92926045, 0.97427653]),\n",
       " 'split4_test_score': array([0.625     , 0.625     , 0.625     , 0.61111111, 0.63888889,\n",
       "        0.63888889, 0.63888889, 0.63888889, 0.66666667, 0.66666667,\n",
       "        0.68055556, 0.66666667, 0.65277778, 0.65277778, 0.65277778,\n",
       "        0.63888889, 0.63888889, 0.63888889, 0.63888889, 0.63888889,\n",
       "        0.63888889, 0.63888889, 0.65277778, 0.65277778, 0.66666667,\n",
       "        0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n",
       "        0.66666667, 0.65277778, 0.63888889, 0.63888889, 0.625     ,\n",
       "        0.63888889, 0.63888889, 0.63888889, 0.65277778, 0.68055556,\n",
       "        0.66666667, 0.66666667, 0.66666667, 0.68055556, 0.66666667,\n",
       "        0.66666667, 0.68055556, 0.68055556, 0.63888889, 0.63888889,\n",
       "        0.625     , 0.63888889, 0.63888889, 0.63888889, 0.65277778,\n",
       "        0.68055556, 0.66666667, 0.66666667, 0.66666667, 0.68055556,\n",
       "        0.66666667, 0.66666667, 0.68055556, 0.68055556, 0.63888889,\n",
       "        0.63888889, 0.625     , 0.65277778, 0.63888889, 0.63888889,\n",
       "        0.65277778, 0.68055556, 0.66666667, 0.66666667, 0.66666667,\n",
       "        0.68055556, 0.66666667, 0.66666667, 0.68055556, 0.68055556]),\n",
       " 'split4_train_score': array([0.81410256, 0.83012821, 0.88141026, 0.91666667, 0.81730769,\n",
       "        0.84935897, 0.8974359 , 0.91987179, 0.83333333, 0.84935897,\n",
       "        0.91025641, 0.91987179, 0.84615385, 0.85576923, 0.90384615,\n",
       "        0.91346154, 0.81089744, 0.83653846, 0.91025641, 0.94551282,\n",
       "        0.82692308, 0.86858974, 0.92307692, 0.94871795, 0.84615385,\n",
       "        0.86858974, 0.91987179, 0.95192308, 0.84615385, 0.87820513,\n",
       "        0.92628205, 0.95512821, 0.81089744, 0.84294872, 0.91346154,\n",
       "        0.95833333, 0.83333333, 0.86858974, 0.91987179, 0.96794872,\n",
       "        0.84615385, 0.87179487, 0.92628205, 0.97115385, 0.84615385,\n",
       "        0.875     , 0.92628205, 0.97115385, 0.81089744, 0.84294872,\n",
       "        0.91025641, 0.96474359, 0.83333333, 0.86858974, 0.91987179,\n",
       "        0.97115385, 0.84615385, 0.86858974, 0.92628205, 0.98076923,\n",
       "        0.84615385, 0.875     , 0.92628205, 0.98076923, 0.81089744,\n",
       "        0.84294872, 0.91346154, 0.97115385, 0.83333333, 0.86858974,\n",
       "        0.91987179, 0.98076923, 0.84615385, 0.86858974, 0.92628205,\n",
       "        0.98717949, 0.84615385, 0.875     , 0.92628205, 0.98717949]),\n",
       " 'std_fit_time': array([0.01174084, 0.04154671, 0.01313575, 0.01907088, 0.07020865,\n",
       "        0.0155107 , 0.00470961, 0.00732586, 0.00768809, 0.00602712,\n",
       "        0.04699849, 0.03968458, 0.03969272, 0.15357155, 0.04333036,\n",
       "        0.07588349, 0.05088442, 0.0106604 , 0.00786523, 0.04951133,\n",
       "        0.00406223, 0.0076949 , 0.05390629, 0.05966282, 0.04327742,\n",
       "        0.05587586, 0.04904674, 0.03969389, 0.05088329, 0.1136574 ,\n",
       "        0.07752397, 0.04978813, 0.00773664, 0.07077071, 0.01057413,\n",
       "        0.06583177, 0.07175342, 0.01139693, 0.0626223 , 0.0122947 ,\n",
       "        0.10965659, 0.08003374, 0.16309177, 0.15326953, 0.12512868,\n",
       "        0.03173975, 0.08105356, 0.07219172, 0.05147366, 0.03342734,\n",
       "        0.04428023, 0.01323732, 0.01714121, 0.03516453, 0.0390533 ,\n",
       "        0.04817949, 0.10677183, 0.05993758, 0.08869205, 0.04564497,\n",
       "        0.09700295, 0.06981068, 0.08175291, 0.04617057, 0.00795735,\n",
       "        0.0428448 , 0.00654694, 0.04565394, 0.06760537, 0.06733127,\n",
       "        0.01079858, 0.01190712, 0.05492429, 0.06047797, 0.06252376,\n",
       "        0.04275227, 0.07028249, 0.09869525, 0.11610549, 0.06498074]),\n",
       " 'std_score_time': array([0.00176651, 0.00249881, 0.00257393, 0.00373802, 0.00230726,\n",
       "        0.04119403, 0.00041903, 0.00069438, 0.00251501, 0.00160188,\n",
       "        0.00201115, 0.00162589, 0.00285095, 0.00282314, 0.00261835,\n",
       "        0.00015962, 0.00053573, 0.00067376, 0.00044998, 0.00069639,\n",
       "        0.000505  , 0.00181738, 0.00233906, 0.00068124, 0.00039521,\n",
       "        0.00046208, 0.00059475, 0.00053043, 0.00168432, 0.00281253,\n",
       "        0.00115723, 0.00132586, 0.00125484, 0.00197403, 0.00235278,\n",
       "        0.00116947, 0.00061211, 0.00294214, 0.04148673, 0.00255756,\n",
       "        0.00165155, 0.00213659, 0.0013956 , 0.00062257, 0.00239284,\n",
       "        0.00143596, 0.00264882, 0.00216502, 0.00207067, 0.00268401,\n",
       "        0.00185635, 0.00070176, 0.00043477, 0.00266508, 0.00255025,\n",
       "        0.0016395 , 0.00214316, 0.00108628, 0.00240804, 0.00149973,\n",
       "        0.00159776, 0.00139766, 0.00232695, 0.00176137, 0.00069993,\n",
       "        0.04169027, 0.00085083, 0.00208976, 0.0024233 , 0.00148488,\n",
       "        0.00257593, 0.00151851, 0.00014754, 0.00119773, 0.00277059,\n",
       "        0.04250899, 0.04091963, 0.00249235, 0.00234965, 0.00190584]),\n",
       " 'std_test_score': array([0.03571527, 0.03292727, 0.03292727, 0.02971557, 0.02957976,\n",
       "        0.0296545 , 0.02650305, 0.02657971, 0.03528599, 0.0351512 ,\n",
       "        0.0430582 , 0.03029729, 0.03321331, 0.03373644, 0.0334777 ,\n",
       "        0.02508782, 0.03666292, 0.03666292, 0.03254561, 0.02846992,\n",
       "        0.02553741, 0.0317222 , 0.02292106, 0.02391092, 0.03528599,\n",
       "        0.03819303, 0.03799403, 0.03397473, 0.03819303, 0.03769241,\n",
       "        0.037461  , 0.0294136 , 0.03666292, 0.03459148, 0.02819899,\n",
       "        0.02274891, 0.03122663, 0.02657971, 0.02158938, 0.03423775,\n",
       "        0.03528599, 0.03806853, 0.03434054, 0.03840223, 0.03819303,\n",
       "        0.03769241, 0.04176926, 0.04170137, 0.03666292, 0.03642166,\n",
       "        0.02827104, 0.02039793, 0.03122663, 0.02657971, 0.02158938,\n",
       "        0.03486829, 0.03528599, 0.03806853, 0.03434054, 0.03454524,\n",
       "        0.03819303, 0.03769241, 0.04176926, 0.04327241, 0.03666292,\n",
       "        0.03642166, 0.02827104, 0.02761826, 0.03122663, 0.02657971,\n",
       "        0.02158938, 0.03486829, 0.03528599, 0.03806853, 0.03434054,\n",
       "        0.03402764, 0.03819303, 0.03769241, 0.04176926, 0.04361626]),\n",
       " 'std_train_score': array([0.004255  , 0.00370305, 0.01200348, 0.00971381, 0.01089547,\n",
       "        0.0123131 , 0.00806659, 0.01326768, 0.01359316, 0.01355571,\n",
       "        0.01334298, 0.01626596, 0.0125922 , 0.01563391, 0.01576758,\n",
       "        0.01869567, 0.007339  , 0.0079385 , 0.00914139, 0.00784213,\n",
       "        0.0122598 , 0.01218779, 0.00902687, 0.00989882, 0.01072356,\n",
       "        0.00891151, 0.01088847, 0.01172516, 0.01417275, 0.01042013,\n",
       "        0.00939872, 0.00936029, 0.00760306, 0.00852115, 0.00766453,\n",
       "        0.00756415, 0.00888717, 0.0083158 , 0.01034602, 0.00785419,\n",
       "        0.01072356, 0.0073987 , 0.00714206, 0.00719278, 0.01409473,\n",
       "        0.01028456, 0.0102609 , 0.00648361, 0.00760306, 0.00852115,\n",
       "        0.00763853, 0.00941806, 0.00877555, 0.0083158 , 0.00915361,\n",
       "        0.01045199, 0.01072356, 0.00833596, 0.00696934, 0.01031045,\n",
       "        0.01409473, 0.01028456, 0.0102609 , 0.00899296, 0.00760306,\n",
       "        0.00852115, 0.00796911, 0.00734885, 0.00877555, 0.0083158 ,\n",
       "        0.00915361, 0.00691174, 0.01072356, 0.00833596, 0.00696934,\n",
       "        0.00597824, 0.01409473, 0.01028456, 0.0102609 , 0.00772675])}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 12, 'max_features': 50, 'max_leaf_nodes': 50}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameters : max_features=50, max_depth=12, max_leaf_nodes=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=12, max_features=50, max_leaf_nodes=50,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=300, criterion='gini',max_features=50,max_depth=12,max_leaf_nodes=50)\n",
    "rf_clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.9557291666666666\n",
      "Test Recall score: 0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = rf_clf.predict(X_train_pca)\n",
    "y_pred_test = rf_clf.predict(X_test_pca)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is also overfitting the model after applying PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Results have improved after using PCA. We get the best resulting using Linear SVC after applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
